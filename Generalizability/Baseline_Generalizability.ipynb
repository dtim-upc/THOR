{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "isColab = True\n",
        "try:\n",
        "  # running on CoLAB Hosted runtime\n",
        "  print(os.environ['HOSTNAME'])\n",
        "  print('Running on CoLAB Hosted runtime.')\n",
        "except:\n",
        "  # running on Local runtime\n",
        "  print('You are running this script on your local machine!')\n",
        "  isColab = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny8ua_Jehm_h",
        "outputId": "eea059d1-8e53-42bb-88cc-ee2079b512a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are running this script on your local machine!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Installing/Upgrading the Libraries Not Available to CoLAB'''\n",
        "if isColab:\n",
        "  !pip install -U 'spacy'==3.6.1\n",
        "  # # Install this if you want to run on Colab GPU\n",
        "  # !pip install -U spacy[cuda-autodetect]\n",
        "  !pip install -U rdflib\n",
        "  !pip install -U spaczz\n",
        "  !pip install nervaluate\n",
        "  !pip install -U print-dict"
      ],
      "metadata": {
        "id": "8Ky2-1coaGjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Downloading Model lg = larger - with word vectors'''\n",
        "if isColab:\n",
        "  !python -m spacy download en_core_web_lg\n",
        "  !python -m spacy download en_core_web_trf\n",
        "\n",
        "  # Adding Word Vectors: https://spacy.io/usage/linguistic-features#adding-vectors\n",
        "  #!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip\n",
        "  #!python -m spacy init vectors en wiki-news-300d-1M-subword.vec.zip /content/wiki-news-300d-1M-subword"
      ],
      "metadata": {
        "id": "irzN7YEJwKjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUhgep3hK-XZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.pipeline import Sentencizer\n",
        "from nervaluate import Evaluator\n",
        "import bs4\n",
        "from print_dict import pd as pdic\n",
        "import requests\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Span\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "from textwrap import wrap\n",
        "import os.path\n",
        "import functools\n",
        "import operator\n",
        "from collections import deque\n",
        "from operator import itemgetter\n",
        "from spacy.symbols import nsubj, VERB, AUX\n",
        "\n",
        "# spaczz lirrary for Similarity Matching\n",
        "from spaczz.matcher import SimilarityMatcher\n",
        "\n",
        "# RDFLib libraries\n",
        "from rdflib import Graph\n",
        "import pprint\n",
        "from rdflib import RDFS\n",
        "from rdflib import URIRef\n",
        "from rdflib.namespace import RDF\n",
        "\n",
        "# Colab specific libraries\n",
        "if isColab:\n",
        "  from google.colab import files\n",
        "\n",
        "# General Libraries\n",
        "import pandas as pd\n",
        "import srsly\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Spacy Related Imports\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding, compile_infix_regex, get_words_and_spaces\n",
        "from spacy.tokens import Span, DocBin, Doc\n",
        "from spacy.vocab import Vocab\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.training import Example\n",
        "# import spacy_sentence_bert\n",
        "\n",
        "# Ignore OOV warnings\n",
        "warnings.filterwarnings(\"ignore\", message=r\"\\[W008\\]\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Disease A-Z Datasets"
      ],
      "metadata": {
        "id": "dNbIXSCaplIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some of the Directories... Please also check the Main function at the bottom\n",
        "COLAB_DIR = \"\"\n",
        "if isColab:\n",
        "  COLAB_DIR = \"/content/\"\n",
        "\n",
        "# SCHEMA_FILE = COLAB_DIR+\"data/Schema/Disease_KG_Extended.ttl\"\n",
        "STRUCTURED_DATA_DIR = COLAB_DIR+\"data/Structured_Data\"\n",
        "MATCHER_DIR = COLAB_DIR+\"data/Matcher\""
      ],
      "metadata": {
        "id": "dP5QwTYkH7J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/data\"\n",
        "if not os.path.exists(data_path):\n",
        "  '''Downloading the Dataset into CoLab temporary directory'''\n",
        "  !mkdir /content/data\n",
        "\n",
        "  # Comment this out when you are not manually providing the dataset\n",
        "  !mkdir /content/data/Annotated_Text\n",
        "  !mkdir /content/data/Annotated_Text/test\n",
        "  !mkdir /content/data/Structured_Data\n",
        "\n",
        "  # !wget -P /content/data/ 'https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/Annotated_Text.zip'\n",
        "  # !wget -P /content/data/ 'https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/Structured_Data.zip'\n",
        "  # !wget -P /content/data/ 'https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/Schema.zip'\n",
        "\n",
        "  # # unzipping the Dataset contents\n",
        "  # !unzip \"/content/data/Annotated_Text.zip\" -d \"/content/data/Annotated_Text/\" && rm \"/content/data/Annotated_Text.zip\"\n",
        "  # !unzip \"/content/data/Structured_Data.zip\" -d \"/content/data/Structured_Data/\" && rm \"/content/data/Structured_Data.zip\"\n",
        "  # !unzip \"/content/data/Schema.zip\" -d \"/content/data/Schema\" && rm \"/content/data/Schema.zip\""
      ],
      "metadata": {
        "id": "x2Ifhur9YGdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168bcc2d-82ea-46b1-bad0-6f5f444b1e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The syntax of the command is incorrect.\n",
            "The syntax of the command is incorrect.\n",
            "The syntax of the command is incorrect.\n",
            "The syntax of the command is incorrect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_out_dir(OUTPUT_DIR):\n",
        "  '''This function will be used to remove the outputs in different run'''\n",
        "  if os.path.exists(OUTPUT_DIR):\n",
        "    shutil.rmtree(OUTPUT_DIR)\n",
        "\n",
        "  os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "RhHDedZfceMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Conversion Part"
      ],
      "metadata": {
        "id": "InQ4__x1UkH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_entity_spans(text, spans):\n",
        "  '''Data Cleaning: Removes leading and trailing white spaces from entity spans.'''\n",
        "  invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "  valid_spans = []\n",
        "  for start, end, label in spans:\n",
        "    valid_start = start\n",
        "    valid_end = end\n",
        "    while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
        "      valid_start += 1\n",
        "    while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
        "      valid_end -= 1\n",
        "\n",
        "    if valid_start < valid_end:\n",
        "      valid_spans.append((valid_start, valid_end, label))\n",
        "\n",
        "  return valid_spans"
      ],
      "metadata": {
        "id": "BCAkc7AYUrnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_entities(entities):\n",
        "  ''''This function will remove overlapping spans'''\n",
        "  entities_copy = entities.copy()\n",
        "\n",
        "  # append entity only if it is longer than its overlapping entity\n",
        "  i = 0\n",
        "  for entity in entities_copy:\n",
        "      j = 0\n",
        "      for overlapping_entity in entities_copy:\n",
        "          # Skip self\n",
        "          if i != j:\n",
        "              e_start, e_end, oe_start, oe_end = entity[0], entity[1], overlapping_entity[0], overlapping_entity[1]\n",
        "              # Delete any entity that overlaps, keep if longer\n",
        "              if ((e_start >= oe_start and e_start <= oe_end) \\\n",
        "              or (e_end <= oe_end and e_end >= oe_start)) \\\n",
        "              and ((e_end - e_start) <= (oe_end - oe_start)):\n",
        "                  entities.remove(entity)\n",
        "          j += 1\n",
        "      i += 1\n",
        "\n",
        "  return entities"
      ],
      "metadata": {
        "id": "ZVMYRe51Qd2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def docanno_to_spacy_ner_db(DATA_DIR):\n",
        "  '''\n",
        "  This function takes a directory of docanno annotated datasets for NER/RE\n",
        "  and converts them into SpaCy DocBin Object which is Trainable via commandline\n",
        "  '''\n",
        "  # Creates a blank Tokenizer with just the English vocab\n",
        "  nlp = spacy.blank(\"en\")\n",
        "\n",
        "  Doc.set_extension(\"rel\", default={},force=True)\n",
        "  vocab = Vocab()\n",
        "\n",
        "  word_count = 0\n",
        "  no_files = 0\n",
        "  no_doc = 0\n",
        "  no_entities = 0\n",
        "  error_cnt = 0\n",
        "\n",
        "  # the DocBin will store the example documents\n",
        "  db = DocBin()\n",
        "\n",
        "  for dirname, _, filenames in os.walk(DATA_DIR):\n",
        "    for filename in filenames:\n",
        "      file_path = os.path.join(dirname, filename)\n",
        "\n",
        "      try:\n",
        "        \"\"\" Iterate through the Jsonl file to create serialize Docbin object / .spacy IOB File \"\"\"\n",
        "        for json_line in srsly.read_jsonl(file_path):\n",
        "\n",
        "          # parsing the docanno JSON data (per-line)\n",
        "          text = json_line[\"text\"]\n",
        "          entities = json_line[\"label\"]\n",
        "          id = json_line[\"id\"]\n",
        "\n",
        "          entities = clean_entities(entities)\n",
        "\n",
        "          new_spans = []\n",
        "          for span in entities:\n",
        "            new_spans.append((span[0], span[1], span[2]))\n",
        "\n",
        "          # cleaning and validating the leading and trailing spaces from the annotated entities\n",
        "          valid_spans = trim_entity_spans(text, new_spans)\n",
        "\n",
        "          \"\"\" Parsing tokens from Text \"\"\"\n",
        "          tokens = nlp(text)\n",
        "\n",
        "          entities = []\n",
        "\n",
        "          spaces = [True if tok.whitespace_ else False for tok in tokens]\n",
        "          words = [t.text for t in tokens]\n",
        "          doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "          for start, end, label in valid_spans:\n",
        "            \"\"\" The modes should be: strict, contract, and expand \"\"\"\n",
        "            # print(eg['text'][int(span[\"start_offset\"]):int(span[\"end_offset\"])])\n",
        "            entity = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
        "\n",
        "            # Not considering the spans which are Erroneous\n",
        "            if entity is None:\n",
        "              error_cnt += 1\n",
        "              # print(tokens)\n",
        "              # file_name = ttext.split('\\n')[0]\n",
        "              # print(f'Error Found in ID: {id} -- Span: {(start, end, label)} -- Part: {tokens[start: end]}')\n",
        "\n",
        "            else:\n",
        "              no_entities += 1\n",
        "              entities.append(entity)\n",
        "\n",
        "          # print(entities)\n",
        "          try:\n",
        "            doc.ents = entities\n",
        "            word_count += len(words)-int(7)\n",
        "          except:\n",
        "            print()\n",
        "            print(entities)\n",
        "            print(f\"Cannot Assign Entities for ID: {id}\")\n",
        "            continue\n",
        "\n",
        "          db.add(doc)\n",
        "          no_doc += 1\n",
        "\n",
        "      except:\n",
        "        print('Error While Loading JSON Data From Input Directory. Please check if you have other file type...')\n",
        "\n",
        "      no_files +=1\n",
        "  print(f\"- Files: {no_files} \\n- Processed Documents: {no_doc} \\n- Total Entities: {no_entities} \\n- Erroneous Entities (Ignored): {error_cnt} \\n- Total Words: {word_count}\")\n",
        "\n",
        "  return doc, db"
      ],
      "metadata": {
        "id": "s18Rxcy3UuSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_from_docbin(file_path: str, nlp):\n",
        "  '''This function loads data from SpaCy docbin formatted files into spacy compitable JSON format'''\n",
        "  doc_bin = DocBin().from_disk(file_path)\n",
        "  samples, entities_count = [], 0\n",
        "  for doc in doc_bin.get_docs(nlp.vocab):\n",
        "    sample = {\n",
        "      \"text\": doc.text,\n",
        "      \"entities\": []\n",
        "    }\n",
        "    if len(doc.ents) > 0:\n",
        "      # print(doc.ents)\n",
        "      entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "      sample[\"entities\"] = entities\n",
        "      entities_count += len(entities)\n",
        "    else:\n",
        "      warnings.warn(\"Sample without entities!\")\n",
        "    samples.append(sample)\n",
        "  return samples, entities_count"
      ],
      "metadata": {
        "id": "5ygazl30VHTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading Data"
      ],
      "metadata": {
        "id": "EpRbrkfFjU8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# '''Import the appropriate model and adding the extra pipeline needed'''\n",
        "# # import en_core_web_md\n",
        "import en_core_web_lg\n",
        "\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "# # sbd = nlp.create_pipe('sentencizer')\n",
        "nlp.add_pipe('sentencizer')"
      ],
      "metadata": {
        "id": "qjgX9rbuBqQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896bf5b1-bc08-41fe-beb0-a9f953450014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x201c3a93bc0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rdf_graph(file_name=\"\"):\n",
        "  '''Getting the RDF file from the user (.nt/.ttl/.xml etc)'''\n",
        "\n",
        "  # parsing the graph\n",
        "  g = Graph()\n",
        "  g.parse(file_name)\n",
        "\n",
        "  # # Loop through some of the triples in the graph (subj, pred, obj)\n",
        "  # print('\\nTotal Triples Found = {}\\n'.format(len(g)))\n",
        "  # print('First 10 Triples:')\n",
        "  # for triple in list(g)[:10]:\n",
        "  #     # Check if there is at least one triple in the Graph\n",
        "  #     # if (subj, pred, obj) not in g:\n",
        "  #     #    raise Exception(\"It better be!\")\n",
        "  #     print(triple)\n",
        "\n",
        "  return g"
      ],
      "metadata": {
        "id": "IBlyu7fRPr6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_templates(g):\n",
        "  '''Given a RDF graph 'g' this function will return a list of (S, P, O) triples having only names'''\n",
        "\n",
        "  # Getting all the unique S, P, O from the graph using the RDFS.domain and RDFS.range\n",
        "  preds_subs = list(g.subject_objects(predicate=RDFS.domain))\n",
        "  preds_objs = list(g.subject_objects(predicate=RDFS.range))\n",
        "\n",
        "  # Getting all the subclasses with corresponding superclasses\n",
        "  sup_sub = g.subject_objects(predicate=RDFS.subClassOf)\n",
        "\n",
        "  # dictionary having a superclasses and it's subclasses {'Treatment':['Medicine', 'Precaution', 'Surgery']}\n",
        "  sup_sub_dic = {}\n",
        "\n",
        "  # populating the dictionary from the graph\n",
        "  for sc in sup_sub:\n",
        "    subj = sc[0].split('#')[1]\n",
        "    obj = sc[1].split('#')[1]\n",
        "\n",
        "    if obj in sup_sub_dic:\n",
        "      sup_sub_dic[obj].append(subj)\n",
        "    else:\n",
        "      sup_sub_dic[obj] = [subj]\n",
        "\n",
        "  # dictionary having a structure {'P':['S', 'O']}\n",
        "  dic_triples = {}\n",
        "\n",
        "  # gets only the name of Predicates and Subjects splitting from the URI's\n",
        "  for ps in preds_subs:\n",
        "    pred = ps[0].split('#')[1]\n",
        "    subj = ps[1].split('#')[1]\n",
        "    dic_triples[pred] = [subj]\n",
        "    # # print(subj, pred)\n",
        "\n",
        "  # matches the Subjects having specific Predicates with Objects\n",
        "  for po in preds_objs:\n",
        "    pred = po[0].split('#')[1]\n",
        "    obj = po[1].split('#')[1]\n",
        "    dic_triples[pred].append(obj)\n",
        "    # # print(pred, obj)\n",
        "\n",
        "  # saves the triples from the dictionary into a list of tuple -> [(S, P, O)]\n",
        "  triples_name = []\n",
        "  for pred in dic_triples:\n",
        "    subj = dic_triples[pred][0]\n",
        "    obj = dic_triples[pred][1]\n",
        "\n",
        "    # checking if the subject is a superclass - we ignore the superclass and only consider it's subclasses\n",
        "    if subj in sup_sub_dic:\n",
        "      # copy it's predicate to all it's subclasses (sub_cls) along with the range\n",
        "      for sub_cls in sup_sub_dic[subj]:\n",
        "        triples_name.append((sub_cls, pred, obj))\n",
        "    else:\n",
        "      triples_name.append((subj, pred, obj))\n",
        "      # print('({}, {}, {})'.format(subj, pred, obj))\n",
        "\n",
        "  # print(\"Total Templates = {}\\n\".format(len(triples_name)))\n",
        "  # for triple in triples_name:\n",
        "  #   print(triple)\n",
        "\n",
        "  return triples_name"
      ],
      "metadata": {
        "id": "9iqjhWhuPthP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # for local runtime - upload the file (first time) in the file upload option (left)\n",
        "# graph = get_rdf_graph(file_name = SCHEMA_FILE)\n",
        "\n",
        "# templates = get_templates(graph)\n",
        "# # Keeping only those templates/triples that has Object Type Property as it's Object\n",
        "# print(f'Extracted a Tolta of {len(templates)} Relationship Templates Having Only Object Properties from the Knowledge Graph:\\n')\n",
        "# templates = [template for template in templates if template[2] != 'string']\n",
        "# pdic(templates)"
      ],
      "metadata": {
        "id": "YRl81M6fQSWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\" This is just to SHOW the Concepts\"\"\"\n",
        "# unique_entities = set()\n",
        "\n",
        "# # Extracting unique subjects and objects from the list\n",
        "# for sub, pred, obj in templates:\n",
        "#   unique_entities.add(sub)\n",
        "#   unique_entities.add(obj)\n",
        "\n",
        "# ENTITY_LABELS = list(unique_entities)\n",
        "# print(ENTITY_LABELS)"
      ],
      "metadata": {
        "id": "sIZD1ccBUnLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ICDE-EX: This is the changes we are doing in order to track down if concept with semantics plays any role or not\n",
        "# # Mapping from ENTITY_LABELS to ENTITY_ATTR\n",
        "# entity_to_attr_map = {label.split('_')[0]: f\"attr{i+1}\" for i, label in enumerate(ENTITY_LABELS)}\n",
        "\n",
        "# # Mapping from ENTITY_ATTR to ENTITY_LABELS (reverse)\n",
        "# attr_to_entity_map = {v: k for k, v in entity_to_attr_map.items()}"
      ],
      "metadata": {
        "id": "YOdZBIrDL64B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENTITY_LABELS = [\"AWARDS\",\n",
        "                 \"CERTIFICATION\",\n",
        "                 \"COLLEGE NAME\",\n",
        "                 \"COMPANIES WORKED AT\",\n",
        "                 \"DEGREE\",\n",
        "                 \"LANGUAGE\",\n",
        "                 \"LOCATION\",\n",
        "                 \"NAME\",\n",
        "                 \"SKILLS\",\n",
        "                 \"UNIVERSITY\",\n",
        "                 \"WORKED AS\",\n",
        "                 \"YEARS OF EXPERIENCE\"]"
      ],
      "metadata": {
        "id": "LzB0QsaO6MfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(entity_to_attr_map)\n",
        "# print(entity_to_attr_map['Symptom'])\n",
        "# print(attr_to_entity_map['attr1'])"
      ],
      "metadata": {
        "id": "1EeFKA0WNrik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_file(file_name=\"\"):\n",
        "  '''This function uploads a file and retunrs it's filename'''\n",
        "\n",
        "  # if file_name is empty, that means we are using CoLAB Google Hosted Runtime\n",
        "  if not file_name:\n",
        "    input_file = files.upload()\n",
        "    file_name = list(input_file.keys())[0]\n",
        "\n",
        "  # removing the new file if already exits\n",
        "  path = os.path.abspath(file_name.split('.')[0]+' (1).'+file_name.split('.')[1])\n",
        "  if os.path.exists(path) == True:\n",
        "    os.remove(path)\n",
        "\n",
        "  return file_name"
      ],
      "metadata": {
        "id": "0AP7eo4e1JiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_csv(file_name=\"\"):\n",
        "  '''\n",
        "  This function will get a CSV from the user having TWO COL i.e., Disease_Name, Affected_Anatomy and\n",
        "  returns a Dictionary having a structure: {\"Tuberculosis\": ['lungs', 'brain', 'kidneys', 'spine'], ...}\n",
        "  '''\n",
        "  # if file_name is empty, that means we are using CoLAB Google Hosted Runtime\n",
        "  if not file_name:\n",
        "    file_name = input_file()\n",
        "\n",
        "  structured_data = {}\n",
        "\n",
        "  with open(file_name, 'r', encoding='ISO-8859-1') as file:\n",
        "      csvreader = csv.reader(file)\n",
        "      # we can ignore the header\n",
        "      header = next(csvreader)\n",
        "      # print('Reading New Structured Data Source: {}'.format(file_name.split('\\\\')[1]))\n",
        "      # print('Data Headers: ', header)\n",
        "\n",
        "      for row in csvreader:\n",
        "        # # print(row)\n",
        "        # splits the comma separated values from the 1st column\n",
        "        first_cols = row[0].split(',')\n",
        "\n",
        "        # each of the instances of the first col (subject) will have the same value domain\n",
        "        for instance in first_cols:\n",
        "          instance = instance.strip()\n",
        "\n",
        "          # if we do not have any second column (non-relational data)\n",
        "          try:\n",
        "            # splits the comma separated values from the 2nd column\n",
        "            value_domain = row[1].split(',')\n",
        "            # removes the leading and trailing spaces\n",
        "            value_domain = [x.strip() for x in value_domain]\n",
        "            # making instance/value-domain dictionary\n",
        "            structured_data[instance] = value_domain\n",
        "          except:\n",
        "            structured_data[instance] = []\n",
        "\n",
        "  # print('Instances/Values:')\n",
        "  # print(structured_data)\n",
        "  # print()\n",
        "\n",
        "  return header, structured_data"
      ],
      "metadata": {
        "id": "SQeMzNp80ssQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_structured_data(STRUCTURED_DATA_DIR=STRUCTURED_DATA_DIR):\n",
        "  '''This will read the Structured Data From the CSV Files of a Directory and accumulate them'''\n",
        "  # DATA: gets the structured data from CSV files\n",
        "  accu_data = {}\n",
        "\n",
        "  for dirname, _, filenames in os.walk(STRUCTURED_DATA_DIR):\n",
        "    for filename in filenames:\n",
        "      file_path = os.path.join(dirname, filename)\n",
        "      # # print(file_path)\n",
        "      sd_header, sd_data = get_data_from_csv(file_path)\n",
        "      accu_data = accumulate_unique_data(prev_data=accu_data, keys_list=sd_header, data_dic=sd_data)\n",
        "\n",
        "  # print('\\nAfter Structured Data Accumulation:')\n",
        "  # print(accu_data)\n",
        "\n",
        "  return accu_data"
      ],
      "metadata": {
        "id": "zLTZuSuozoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(file_name=\"\"):\n",
        "  '''Getting the TEXT file from the user - and renaming it'''\n",
        "\n",
        "  # if file_name is empty, that means we are using CoLAB Google Hosted Runtime\n",
        "  input_txt = \"\"\n",
        "  if not file_name:\n",
        "    text_file = files.upload()\n",
        "    file_name = list(text_file.keys())[0]\n",
        "    input_txt = text_file[file_name].decode(\"utf-8\").strip()\n",
        "\n",
        "    given_text_file_name = str(file_name)\n",
        "\n",
        "    old_name = COLAB_DIR + given_text_file_name\n",
        "    new_name = COLAB_DIR + 'inference_text.txt'\n",
        "\n",
        "    # removing the new file if already exits\n",
        "    if os.path.exists(new_name) == True:\n",
        "      os.remove(new_name)\n",
        "\n",
        "    os.rename(old_name, new_name)\n",
        "    file_name = new_name\n",
        "\n",
        "  # if the file is called from local runtime with filename specified\n",
        "\n",
        "  with open(file_name, 'r', encoding='utf-8') as text_file:\n",
        "    input_txt = text_file.read().strip()\n",
        "\n",
        "  # print(\"Contents of the Input Text File:\")\n",
        "  # print(input_txt)\n",
        "\n",
        "  return input_txt"
      ],
      "metadata": {
        "id": "jNhRCQEkMFl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_segmentation(input_txt, print_indx = False):\n",
        "    '''This Function Segments the Text into Sentences and Tokens'''\n",
        "\n",
        "    doc = nlp(input_txt)\n",
        "\n",
        "    sentences = []\n",
        "    # sentences_tokens = []\n",
        "    for sent in doc.sents:\n",
        "        # print(sent)\n",
        "        # here sentences are strings\n",
        "        sentences.append(sent.text.strip())\n",
        "        # here a  particular sentence is a list of strings containing the words/tokens\n",
        "        # sentences_tokens.append([token.text for token in sent])\n",
        "\n",
        "    # print('\\nTotal Sentence Found = ', len(sentences))\n",
        "\n",
        "    if print_indx is not False:\n",
        "        print('\\nExample Sentence and Tokens:')\n",
        "        print(sentences[print_indx])\n",
        "        # print(sentences_tokens[print_indx])\n",
        "\n",
        "    return doc, sentences"
      ],
      "metadata": {
        "id": "i-zCn0yMRKqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptualization"
      ],
      "metadata": {
        "id": "ktC0mpAf6rw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_unique_data(prev_data, keys_list, data_dic):\n",
        "  ''' This function creates a set (unique lists) of data for each of the Concepts (e.g., Disease, Sysmtoms, Anatomy etc.)\n",
        "      Parameters:\n",
        "        pre_data  -> already created dictionary with other data sources previously\n",
        "        keys_list -> list of keys (column names) for the new data source\n",
        "        data_dic  -> a dictionary holding the source\n",
        "\n",
        "      Returns:  a dictionary having the keys_list as key and a list of unique (set) values from the data source\n",
        "  '''\n",
        "  new_data = {}\n",
        "  # if we have data already residing in the prev_dic\n",
        "  if prev_data:\n",
        "    new_data = prev_data\n",
        "\n",
        "  # get a list from the keys of the data_dic - for example list of disease names\n",
        "  keys_data_list = list(data_dic.keys())\n",
        "\n",
        "  values_data_list = list(data_dic.values())\n",
        "  # flattening the list of lists and taking only a the unique values\n",
        "  values_data_list = list(set(functools.reduce(operator.iconcat, values_data_list)))\n",
        "\n",
        "  # merging the above tow in a single list of lists\n",
        "  data_lists = [keys_data_list, values_data_list]\n",
        "\n",
        "  # Accumulates the new data into Key and Value while merging prev values if the key is already present\n",
        "  for key, val in zip(keys_list, data_lists):\n",
        "    if key in new_data:\n",
        "      pre_val = new_data[key]\n",
        "      # # print(pre_val)\n",
        "      val = list(set(functools.reduce(operator.iconcat, [val + pre_val])))\n",
        "      # # print(val)\n",
        "\n",
        "    # removing empty string from the instances\n",
        "    if '' in val:\n",
        "      val.remove('')\n",
        "\n",
        "    new_data[key] = val\n",
        "\n",
        "  # # print(new_data,'\\n')\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "QcfJcDbZhNoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initiate_matcher(patterns):\n",
        "  '''Initializing and Fine-tuning the matcher'''\n",
        "  # For more information of how it works please see: https://github.com/gandersen101/spaczz\n",
        "  # changing min_r2 from default of 75 to produce matches in this example -- using custom vocab doesn't work for now\n",
        "  matcher = SimilarityMatcher(vocab=nlp.vocab, min_r2=100, thresh=100)\n",
        "\n",
        "  # UNCOMMENT - this if you set the threshold level to below 50\n",
        "  # matcher = SimilarityMatcher(vocab=nlp.vocab, min_r2=threshold, min_r1=1)\n",
        "\n",
        "  for key_concept in patterns:\n",
        "    # print(key_concept)\n",
        "\n",
        "    pattern = [nlp(instance.lower()) for instance in patterns[key_concept]]\n",
        "\n",
        "    # we can add as many patterns as we want with specified names of Keys/Concepts\n",
        "    matcher.add(key_concept, pattern)\n",
        "\n",
        "  return matcher"
      ],
      "metadata": {
        "id": "QsGH_roxCIwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_noun_chunks(SENTENCES):\n",
        "  ''' This function iterates over the sentences and splits them into NP (Noun Chunks)\n",
        "  '''\n",
        "  # doc = nlp(\"Typical symptoms of active Tuberculosis are chronic cough with blood-containing mucus, fever, night sweats, and weight loss.\")\n",
        "  # NP per sentence {indx:(sen_indx, [chunk1, chunk1,...])}\n",
        "  np_sentences = {}\n",
        "\n",
        "  for indx, sentence in enumerate(SENTENCES):\n",
        "    # print(\"\\nSENTENCE-{}: {}\\n\".format(indx, sentence))\n",
        "    # This is using Transformer Based Model to generate better Dependency Parsing.\n",
        "    sent = nlp(sentence)\n",
        "\n",
        "    # Extracts noun chunks possibly with leading and trailing stop words from the sentence\n",
        "    np_chunks = [chunk for chunk in sent.noun_chunks]\n",
        "\n",
        "    np_sentences[indx] = (sent, np_chunks)\n",
        "\n",
        "  return np_sentences"
      ],
      "metadata": {
        "id": "I3-e6OBFvPah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_dup_match(matches):\n",
        "  '''This function will remove the matched instances where they have same span and entity type but different confidence score\n",
        "    Example:\n",
        "      (abdomen, 35, 36, Anatomy, 1.0)\n",
        "      (abdomen, 35, 36, Anatomy, 0.5)\n",
        "      (abdomen, 35, 36, Anatomy, 0.3)\n",
        "  '''\n",
        "  filtered_dict = {}\n",
        "\n",
        "  for match_tup in matches:\n",
        "    # We'll create a dictionary with keys as a combination of (match_id, start, end) and values as the original tuple.\n",
        "    match_id, start, end, confidence_score, pattern = match_tup\n",
        "    key = (match_id, start, end)\n",
        "\n",
        "    # If we encounter a tuple with the same (match_id, start, end) but a higher confidence_score, we'll update the dictionary value.\n",
        "    if key not in filtered_dict or confidence_score > filtered_dict[key][3]:\n",
        "      filtered_dict[key] = match_tup\n",
        "\n",
        "  return list(filtered_dict.values())"
      ],
      "metadata": {
        "id": "dL5VR9reILbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_matcher(np_sentences, matcher, top_k=1):\n",
        "  ''' Need More Details: This is a Core function that uses Word Vectors internally to match similar concepts...!! '''\n",
        "\n",
        "  # matches per sentence {i:[(np_chunk, matched_sub_chunk, pattern_concept, confidence), ...]}\n",
        "  matched_sentences = {}\n",
        "\n",
        "  for i in np_sentences:\n",
        "\n",
        "    sent, clean_np_chunks = np_sentences[i]\n",
        "    matches_sent = []\n",
        "    # print(f\"\\nSENTENCE-{i+1}: {sent}\")\n",
        "\n",
        "    for chunk in clean_np_chunks:\n",
        "      phrase = nlp(chunk.text)\n",
        "\n",
        "      # TODO: we could apply the Matcher for the whole DOCUMENT - to check **speed improvement**\n",
        "      matches_orig = matcher(phrase)\n",
        "\n",
        "      matches = []\n",
        "\n",
        "      # We go for further ranking (+sorting) using two more matching techniques\n",
        "      for match_id, start, end, ratio, pattern in matches_orig:\n",
        "        confidence_score = round(ratio/100, 2)\n",
        "\n",
        "        # if confidence_score == 1.0 and str(phrase[start:end]) == pattern:\n",
        "          # print(f'{phrase[start:end]} -- {pattern}')\n",
        "          # print(type(phrase[start:end]), type(pattern))\n",
        "        matches.append((match_id, start, end, confidence_score, pattern))\n",
        "\n",
        "      # Removing duplicate matches having same span as well as same entity type (matching id)\n",
        "      matches = remove_dup_match(matches)\n",
        "\n",
        "      # TODO: For now we are only taking the TOP-3 matching results.\n",
        "      for match_id, match_start, match_end, conf, pattern in matches[:top_k]:\n",
        "        # This is to track down the matches within the whole Sentence\n",
        "        sent_start = chunk.start + match_start\n",
        "        sent_end = chunk.start + match_end\n",
        "\n",
        "        try:\n",
        "          matched_sub_chunk = Span(sent, sent_start, sent_end, label=chunk.label)\n",
        "        except:\n",
        "          print(f'Error Creating Span in Sentence {i}\\n{sent}')\n",
        "          continue\n",
        "\n",
        "        # TODO: If we need we can also include the NP (chunk) here for future use\n",
        "        matches_sent.append((chunk, matched_sub_chunk, match_id, conf))\n",
        "\n",
        "        # print(f'Chunk = {chunk}, Match Start = {match_start}, Match End = {match_end}, Matched Sub-Chunk = {matched_sub_chunk}, Concept = {match_id}, Confidence = {conf}, Pattern = {pattern}')\n",
        "\n",
        "    # Adding the (sent, matches) to the dictionary with Sentence Index as Key for return purpose\n",
        "    matched_sentences[i] = (sent, matches_sent)\n",
        "\n",
        "  return matched_sentences"
      ],
      "metadata": {
        "id": "eu2ENipU-OVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_match(matched_sentences):\n",
        "  # print('Final List of Concepts in this Sentence:\\n')\n",
        "\n",
        "  count = 0\n",
        "  for S_i in matched_sentences:\n",
        "    sent, matches = matched_sentences[S_i]\n",
        "\n",
        "    print(\"Sentence-{}: '{}'\".format(S_i, sent))\n",
        "    # print(matches)\n",
        "\n",
        "    for np_chunk, matched_sub_chunk, match_id, conf in matches:\n",
        "      print(f'\\t\\tNP Chunk = {np_chunk}, Matched Sub-Chunk = {matched_sub_chunk}, Match Start = {matched_sub_chunk.start}, Match End = {matched_sub_chunk.end}, Concept = {match_id}, Confidence = {conf}')\n",
        "      count += 1\n",
        "    print()\n",
        "\n",
        "  print(f'\\nTotal Entities Recognized in Sentences = {count}')"
      ],
      "metadata": {
        "id": "pxWD_Ejyux0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conceptualized_preprocessed_doc(prep_doc, matched_sentences):\n",
        "  ''' This function will map the sentence based matching results into the coreferred documents.\n",
        "      The chunks produces here uses token based span according to the pre-processed document as a whole.\n",
        "\n",
        "      Returns a list of 3 tuples (np_chunk_doc, matched_sub_chunk_doc, match_id, conf)\n",
        "      --> [(Abdominal aortic aneurysm, Abdominal aortic aneurysm, 'Disease', 1.0), (Abdominal aortic aneurysm, Abdominal, 'Anatomy', 0.62), ...]\n",
        "  '''\n",
        "\n",
        "  entities_pre_doc = []\n",
        "  for i, sent in enumerate(prep_doc.sents):\n",
        "    sent_from_matcher, matched_results = matched_sentences[i]\n",
        "\n",
        "    for np_chunk, matched_sub_chunk, match_id, conf in matched_results:\n",
        "      # Adjust the start and end indices of NP Chunk according to the doc\n",
        "      np_chunk_doc_start = sent.start + np_chunk.start\n",
        "      np_chunk_doc_end = sent.start + np_chunk.end\n",
        "\n",
        "      # Adjust the start and end indices of matched sub-chunk according to the doc\n",
        "      sub_chunk_doc_start = sent.start + matched_sub_chunk.start\n",
        "      sub_chunk_doc_end = sent.start + matched_sub_chunk.end\n",
        "\n",
        "      # Making token based span according to the doc\n",
        "      np_chunk_doc = Span(prep_doc, np_chunk_doc_start, np_chunk_doc_end, label=np_chunk.label)\n",
        "      matched_sub_chunk_doc = Span(prep_doc, sub_chunk_doc_start, sub_chunk_doc_end, label=matched_sub_chunk.label)\n",
        "      entities_pre_doc.append((np_chunk_doc, matched_sub_chunk_doc, match_id, conf))\n",
        "\n",
        "      # print(matched_sub_chunk, matched_sub_chunk_doc)\n",
        "\n",
        "  # print(f'\\nTotal Entities Mapped in Document = {len(entities_pre_doc)}')\n",
        "  return entities_pre_doc"
      ],
      "metadata": {
        "id": "3HNW3GBqJZjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_char_span(orig_doc, input_txt, entities_pre_doc, tag_np_chunk_only = False):\n",
        "  '''This function conversts token span into character span for the evaluation script to work.\n",
        "     You can choose either to tag the NP Chunk or to be more granular by tagging with Sub-Chunk level matching (default).\n",
        "     The original document object is also populated with the token span so that we can visualize the results of our prediction\n",
        "  '''\n",
        "\n",
        "  entities_span = []\n",
        "  doc_span = []\n",
        "\n",
        "  if tag_np_chunk_only:\n",
        "    for np_chunk_doc, matched_sub_chunk_doc, match_id, conf in entities_pre_doc:\n",
        "      # This will be used for SemEval Evaluation\n",
        "      entities_span.append((np_chunk_doc.start_char, np_chunk_doc.end_char, match_id))\n",
        "      # This will be used to visualize the Span\n",
        "      doc_span.append(Span(orig_doc, np_chunk_doc.start, np_chunk_doc.end, match_id))\n",
        "  else:\n",
        "    for np_chunk_doc, matched_sub_chunk_doc, match_id, conf in entities_pre_doc:\n",
        "      # This will be used for SemEval Evaluation\n",
        "      entities_span.append((matched_sub_chunk_doc.start_char, matched_sub_chunk_doc.end_char, match_id))\n",
        "      # This will be used to visualize the Span\n",
        "      doc_span.append(Span(orig_doc, matched_sub_chunk_doc.start, matched_sub_chunk_doc.end, match_id))\n",
        "\n",
        "  # A dictionary in the format {'text': 'Tuberculosis generally damages the lungs', 'entities': [(0, 12, 'Disease_E'), (35, 40, 'Anatomy_E')]}\n",
        "  ner_prediction = {'text': input_txt, 'entities': entities_span}\n",
        "  orig_doc.spans[\"sc\"] = doc_span\n",
        "\n",
        "  return orig_doc, ner_prediction"
      ],
      "metadata": {
        "id": "otK3hsiA74t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_spacy_ner_doc(ner_pred):\n",
        "  '''\n",
        "  This function takes a list of directory of NER predictions of the form\n",
        "  {'text': '...', 'entities':[(start, end, tag)]} and converts them into SpaCy Doc Object\n",
        "  '''\n",
        "  # Creates a blank Tokenizer with just the English vocab\n",
        "  nlp = spacy.blank(\"en\")\n",
        "\n",
        "  Doc.set_extension(\"rel\", default={},force=True)\n",
        "  vocab = Vocab()\n",
        "\n",
        "  # try:\n",
        "  # parsing the docanno JSON data (per-line)\n",
        "  text = ner_pred[\"text\"]\n",
        "  spans = ner_pred[\"entities\"]\n",
        "\n",
        "  \"\"\" Parsing tokens from Text \"\"\"\n",
        "  tokens = nlp(text)\n",
        "\n",
        "  entities = []\n",
        "\n",
        "  spaces = [True if tok.whitespace_ else False for tok in tokens]\n",
        "  words = [t.text for t in tokens]\n",
        "  doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "  for start, end, label in spans:\n",
        "    \"\"\" The modes should be: strict, contract, and expand \"\"\"\n",
        "      # print(eg['text'][int(span[\"start_offset\"]):int(span[\"end_offset\"])])\n",
        "    entity = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
        "\n",
        "    # Not considering the spans which are Erroneous\n",
        "    if entity is None:\n",
        "      # file_name = text.split('\\n')[0]\n",
        "      print(f'No Entity Found in File: {file_name};\\n Span = {start}-{end}; Phrase = {doc.text[start:end]}; Label = {label}\\n')\n",
        "      continue\n",
        "    else:\n",
        "      entities.append(entity)\n",
        "\n",
        "  # print(entities[0].label_)\n",
        "  try:\n",
        "    doc.ents = entities\n",
        "  except:\n",
        "    print(\"=>> Error\")\n",
        "    print(text)\n",
        "\n",
        "  # except:\n",
        "  #   print('Error While Loading Predicted List...')\n",
        "\n",
        "  return doc"
      ],
      "metadata": {
        "id": "1rL4VUMArPxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Assigns different colors to the Entities during visualization.'''\n",
        "\n",
        "color_list = ['yellow', 'white', 'orange', '#008000', '#800000', '#0D9CB4', '#5813C7', '#0D350E', '#1AA436',\n",
        "          '#1AE0F9', '#BADCA1', '#78A2E5', '#D845FB', '#54B69E', '#800080', '#FF00FF', '#000080']\n",
        "\n",
        "colors = dict(zip(ENTITY_LABELS, color_list))\n",
        "options = {\"colors\": colors}"
      ],
      "metadata": {
        "id": "uVqNZk4vJRmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_pred_span_cat(doc):\n",
        "  '''Desired Format: doc.spans[\"sc\"] = [Span(doc, 3, 6, \"ORG\"), Span(doc, 5, 6, \"GPE\")]\n",
        "  '''\n",
        "  spacy.displacy.render(doc, style=\"span\", options=options, jupyter=True)"
      ],
      "metadata": {
        "id": "5X5IdFY4A6Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_groud_truth_entities(doc):\n",
        "  spacy.displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
      ],
      "metadata": {
        "id": "0X3Zmm_SJWKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(ner_predictions, filename, semeval_format=True):\n",
        "  # Saving the predictions as JSON - each dictionary on a line\n",
        "  semeval_ent = []\n",
        "  with open(OUTPUT_DIR+'/'+filename, 'w', encoding='UTF-8') as json_file:\n",
        "    for pred in ner_predictions:\n",
        "      if semeval_format:\n",
        "        # prodigy format to work with nereval library - for SemEval 2013 - 9.1 task.\n",
        "        tmp_ent = []\n",
        "        for ent in pred['entities']:\n",
        "          # saved in this format: [{\"label\": \"PER\", \"start\": 2, \"end\": 4}, ... ]\n",
        "          tmp_ent.append({\"label\": ent[2], \"start\": ent[0], \"end\": ent[1]})\n",
        "\n",
        "        semeval_ent.append(tmp_ent)\n",
        "\n",
        "      else:\n",
        "        # this is regullar spacy format, can be used for spacy's default evaluation later\n",
        "        json_file.write(json.dumps(pred, ensure_ascii=False))\n",
        "        json_file.write('\\n')\n",
        "\n",
        "    if semeval_format:\n",
        "      # dumping it into a JSON file\n",
        "      json_file.write(json.dumps(semeval_ent, ensure_ascii=False))\n",
        "\n",
        "  return semeval_ent\n",
        "  # # This is single line JSON Dump of the entile list of dictionary - parser cannot parse it directly\n",
        "  # with open(OUTPUT_DIR+'/predition.jsonl', 'w') as fout:\n",
        "  #     json.dump(ner_predictions, fout)"
      ],
      "metadata": {
        "id": "TKvCpbC5ov7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_results(results_by_tag):\n",
        "  '''This fucntion is used to process the individual entity based scores together'''\n",
        "  results_by_entity = []\n",
        "  for entity in ENTITY_LABELS:\n",
        "    df = pd.DataFrame(results_by_tag[entity])\n",
        "    df = df.round(decimals = 2)\n",
        "    df.insert(0,'Entity','')\n",
        "    df['Entity'] = entity\n",
        "    results_by_entity.append(df)\n",
        "\n",
        "  return results_by_entity"
      ],
      "metadata": {
        "id": "QPUwmNTfu0Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semeval_evaluation(true, pred):\n",
        "    evaluator = Evaluator(true, pred, tags=ENTITY_LABELS)\n",
        "    results, results_by_tag = evaluator.evaluate()\n",
        "\n",
        "    results = pd.DataFrame(results)\n",
        "    results.to_excel(OUTPUT_DIR+'/'+'overall_benchmark.xlsx')\n",
        "\n",
        "    results_by_entity = pd.concat(process_results(results_by_tag))\n",
        "    results_by_entity.to_excel(OUTPUT_DIR+'/'+'entity_benchmark.xlsx')\n",
        "\n",
        "    return results, results_by_entity"
      ],
      "metadata": {
        "id": "6FlqR5Dqt_nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import glob\n",
        "\n",
        "def save_matcher(matcher_file):\n",
        "  with open(matcher_file, 'wb') as out_dir:\n",
        "      pickle.dump(matcher, out_dir, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_matcher(matcher_dir):\n",
        "  matcher_file = glob.glob(os.path.join(matcher_dir, '*.pkl'))[0]\n",
        "  with open(matcher_file, 'rb') as inp_dir:\n",
        "    matcher = pickle.load(inp_dir)\n",
        "  return matcher"
      ],
      "metadata": {
        "id": "GBMfpPd_rX2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "_x4dRIoBxInd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "\n",
        "  # Prompting for evaluation set - validation/test\n",
        "  EVAL_SET = ['valid', 'test', 'train', 'inf']\n",
        "  EVAL_SET = EVAL_SET[int(input('Chose Dataset for Evaluation:\\n[0] Validation\\n[1] Test\\n[2] Training\\n[3] Inferencing\\nEnter your choice:'))]\n",
        "\n",
        "  # Input/Output Directories\n",
        "  INPUT_DIR = \"\"\n",
        "  OUTPUT_DIR = \"\"\n",
        "\n",
        "  if EVAL_SET == 'inf':\n",
        "    if isColab:\n",
        "      input_text = get_text()\n",
        "    else:\n",
        "      # uploaded_text = files.upload()\n",
        "      # text_filename = inference_file(uploaded_text)\n",
        "      input_text = get_text(\"data/Annotated_Text/single.txt\")\n",
        "\n",
        "    samples = [{'text':input_text}]\n",
        "    # print(input_text)\n",
        "  else:\n",
        "    # Input/Output Directories\n",
        "    INPUT_DIR = f'{COLAB_DIR}data/Annotated_Text/{EVAL_SET}'\n",
        "    OUTPUT_DIR = f'{COLAB_DIR}output/{EVAL_SET}'\n",
        "    create_out_dir(OUTPUT_DIR)\n",
        "\n",
        "    print('\\nPreparing Evaluation Dataset:')\n",
        "    doc_valid, db_valid = docanno_to_spacy_ner_db(INPUT_DIR)\n",
        "    db_valid.to_disk(OUTPUT_DIR + f\"/disease_A-Z_{EVAL_SET}.spacy\")\n",
        "\n",
        "    # reading the ground truth entities with text file from spacy docbin\n",
        "    samples, ground_entities_count = load_json_from_docbin(OUTPUT_DIR + f\"/disease_A-Z_{EVAL_SET}.spacy\", nlp)\n",
        "\n",
        "  # reading and accumulating (integrated) structured data\n",
        "  accu_data = accumulate_structured_data()\n",
        "  # pdic(accu_data)\n",
        "  # initializing and fine-tuning the matcher\n",
        "\n",
        "  if not os.path.exists(MATCHER_DIR):\n",
        "    create_out_dir(MATCHER_DIR)\n",
        "    print('Building the patterns...')\n",
        "    matcher = initiate_matcher(patterns=accu_data)\n",
        "\n",
        "    # saving the matcher for future usecases... reducing time\n",
        "    save_matcher(f'{MATCHER_DIR}/matcher_baseline.pkl')\n",
        "\n",
        "  else:\n",
        "    print('Loading the matcher from directory...')\n",
        "    matcher = load_matcher(MATCHER_DIR)\n",
        "\n",
        "  ner_predictions = []\n",
        "  all_docs = []\n",
        "  count = 0\n",
        "  total_doc = len(samples)\n",
        "\n",
        "  print('\\nConceptualization Process Started...')\n",
        "  # each sample represents one document..\n",
        "  for ground in samples:\n",
        "    input_txt = ground['text']\n",
        "    # print(f'\\n{input_txt}')\n",
        "\n",
        "    # Segmenting the document into sentences - for Conceptualization we can also process the whole document\n",
        "    doc, sentences = sentence_segmentation(input_txt, print_indx=False)\n",
        "\n",
        "    # Detecting the NP Chunks - also removes leading and trailing stop words in the NP chunks\n",
        "    np_sentences = identify_noun_chunks(sentences)\n",
        "    # print(np_sentences)\n",
        "\n",
        "    # Getting the initial Top-k matrching results\n",
        "    matched_sentences = baseline_matcher(np_sentences, matcher=matcher, top_k=1)\n",
        "    # print_match(matched_sentences)\n",
        "\n",
        "    # Conceptualizing the Text Document - token based indices\n",
        "    entities_pre_doc = conceptualized_preprocessed_doc(doc, matched_sentences)\n",
        "    # print(f'\\nEntities in the Coreference Resolved Document:\\n{entities_pre_doc}')\n",
        "\n",
        "    # Organizing indexes of those entitties according to the original text, also creating span categories for visualization\n",
        "    doc, ner_pred_tmp = get_char_span(doc, input_txt, entities_pre_doc, tag_np_chunk_only = False)\n",
        "    # print(f'\\nNER Prediction for current document:\\n{ner_pred_tmp}')\n",
        "    ner_predictions.append(ner_pred_tmp)\n",
        "    all_docs.append(doc)\n",
        "\n",
        "    count +=1\n",
        "    # print(f'Document-{count}')\n",
        "    if count % 10 == 0:\n",
        "      print(f'{count}/{total_doc} Document Processed...')\n",
        "\n",
        "  if EVAL_SET == 'inf':\n",
        "    # Visualizing the Predictions for the Given Input Text\n",
        "    print('\\n########### Prediction ###########\\n')\n",
        "    render_pred_span_cat(all_docs[0])\n",
        "\n",
        "  else:\n",
        "    # saving the grond and predictions into a JSONL file for later evaluation.\n",
        "    semeval_ground = save_predictions(samples, filename='ground.jsonl')\n",
        "    semeval_pred = save_predictions(ner_predictions, filename='predition.jsonl')\n",
        "\n",
        "    # doing the evaluation following SemEval 2013 metrics\n",
        "    results, results_by_entity = semeval_evaluation(true=semeval_ground, pred=semeval_pred)\n",
        "\n",
        "    # Saving this for Future Experiments... Spacy Format\n",
        "    _ = save_predictions(ner_predictions, filename='predition_Baseline_spacy.jsonl', semeval_format=False)\n",
        "\n",
        "    print('\\n########### Overall Results ###########\\n')\n",
        "    print(f\"Precision: {results['partial']['precision']}\\nRecall: {results['partial']['recall']}\\nF1: {results['partial']['f1']}\\n\")"
      ],
      "metadata": {
        "id": "1OzJAh7XDTVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b80f242-bd01-4f47-fc5e-8ce451aabae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chose Dataset for Evaluation:\n",
            "[0] Validation\n",
            "[1] Test\n",
            "[2] Training\n",
            "[3] Inferencing\n",
            "Enter your choice:1\n",
            "\n",
            "Preparing Evaluation Dataset:\n",
            "- Files: 1 \n",
            "- Processed Documents: 20 \n",
            "- Total Entities: 2140 \n",
            "- Erroneous Entities (Ignored): 107 \n",
            "- Total Words: 38319\n",
            "Building the patterns...\n",
            "\n",
            "Conceptualization Process Started...\n",
            "10/20 Document Processed...\n",
            "20/20 Document Processed...\n",
            "\n",
            "########### Overall Results ###########\n",
            "\n",
            "Precision: 0.14519056261343014\n",
            "Recall: 0.07476635514018691\n",
            "F1: 0.09870450339296731\n",
            "\n"
          ]
        }
      ]
    }
  ]
}