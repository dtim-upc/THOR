{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtim-upc/THOR/blob/main/Generalizability/THOR_Generalizability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "isColab = True\n",
        "try:\n",
        "  # running on CoLAB Hosted runtime\n",
        "  print(os.environ['HOSTNAME'])\n",
        "  print('Running on CoLAB Hosted runtime.')\n",
        "except:\n",
        "  # running on Local runtime\n",
        "  print('You are running this script on your local machine!')\n",
        "  isColab = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny8ua_Jehm_h",
        "outputId": "9ff8ccbb-7762-41a5-fff2-434dfa62eb10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "881be9c5ffab\n",
            "Running on CoLAB Hosted runtime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Installing/Upgrading the Libraries Not Available to CoLAB'''\n",
        "if isColab:\n",
        "  !pip install -U 'spacy'==3.6.1\n",
        "  # # Install this if you want to run on Colab GPU\n",
        "  # !pip install -U spacy[cuda-autodetect]\n",
        "  !pip install -U rdflib\n",
        "  !pip install -U spaczz\n",
        "  !pip install nervaluate\n",
        "  !pip install -U print-dict\n",
        "  !pip3 install lemminflect"
      ],
      "metadata": {
        "id": "8Ky2-1coaGjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a41dd9-f6fc-4d39-cf08-bf884a8a5efe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3.6.1\n",
            "  Downloading spacy-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (3.0.9)\n",
            "Collecting thinc<8.2.0,>=8.1.8 (from spacy==3.6.1)\n",
            "  Downloading thinc-8.1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (919 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.6/919.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (0.9.0)\n",
            "Collecting pathy>=0.10.0 (from spacy==3.6.1)\n",
            "  Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.6.1) (3.3.0)\n",
            "Collecting pathlib-abc==0.1.1 (from pathy>=0.10.0->spacy==3.6.1)\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.6.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.6.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.6.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.6.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.6.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.6.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.6.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.6.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.6.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.6.1) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.6.1) (2.1.5)\n",
            "Installing collected packages: pathlib-abc, pathy, thinc, spacy\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.3\n",
            "    Uninstalling thinc-8.2.3:\n",
            "      Successfully uninstalled thinc-8.2.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.4\n",
            "    Uninstalling spacy-3.7.4:\n",
            "      Successfully uninstalled spacy-3.7.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pathlib-abc-0.1.1 pathy-0.11.0 spacy-3.6.1 thinc-8.1.12\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-7.0.0\n",
            "Collecting spaczz\n",
            "  Downloading spaczz-0.6.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: catalogue in /usr/local/lib/python3.10/dist-packages (from spaczz) (2.0.10)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from spaczz) (1.25.2)\n",
            "Collecting rapidfuzz>=1.0.0 (from spaczz)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from spaczz) (2023.12.25)\n",
            "Requirement already satisfied: spacy<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from spaczz) (3.6.1)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.10/dist-packages (from spaczz) (2.4.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (1.1.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0,>=3.0->spaczz) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<4.0,>=3.0->spaczz) (0.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0,>=3.0->spaczz) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0,>=3.0->spaczz) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0,>=3.0->spaczz) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spaczz) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spaczz) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spaczz) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spaczz) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0,>=3.0->spaczz) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0,>=3.0->spaczz) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.0,>=3.0->spaczz) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0,>=3.0->spaczz) (2.1.5)\n",
            "Installing collected packages: rapidfuzz, spaczz\n",
            "Successfully installed rapidfuzz-3.6.1 spaczz-0.6.0\n",
            "Collecting nervaluate\n",
            "  Downloading nervaluate-0.1.8-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: nervaluate\n",
            "Successfully installed nervaluate-0.1.8\n",
            "Collecting print-dict\n",
            "  Downloading print_dict-0.1.19-py3-none-any.whl (8.3 kB)\n",
            "Collecting yapf<0.31.0,>=0.30.0 (from print-dict)\n",
            "  Downloading yapf-0.30.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yapf, print-dict\n",
            "Successfully installed print-dict-0.1.19 yapf-0.30.0\n",
            "Collecting lemminflect\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lemminflect) (1.25.2)\n",
            "Installing collected packages: lemminflect\n",
            "Successfully installed lemminflect-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Downloading Model lg = larger - with word vectors'''\n",
        "if isColab:\n",
        "  !python -m spacy download en_core_web_lg\n",
        "  !python -m spacy download en_core_web_trf\n",
        "\n",
        "  # Adding Word Vectors: https://spacy.io/usage/linguistic-features#adding-vectors\n",
        "  #!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip\n",
        "  #!python -m spacy init vectors en wiki-news-300d-1M-subword.vec.zip /content/wiki-news-300d-1M-subword"
      ],
      "metadata": {
        "id": "irzN7YEJwKjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ecec48-2be1-40b2-b240-fb359f67f093"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-05 20:07:23.658487: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-05 20:07:23.658550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-05 20:07:23.660755: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-05 20:07:23.674875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-05 20:07:26.499150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.5)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "2024-03-05 20:08:07.400282: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-05 20:08:07.400349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-05 20:08:07.401830: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-05 20:08:07.410067: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-05 20:08:08.687394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-trf==3.6.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.6.1/en_core_web_trf-3.6.1-py3-none-any.whl (460.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.3/460.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-trf==3.6.1) (3.6.1)\n",
            "Collecting spacy-transformers<1.3.0,>=1.2.2 (from en-core-web-trf==3.6.1)\n",
            "  Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.8/190.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (4.66.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.3.0)\n",
            "Collecting transformers<4.31.0,>=3.4.0 (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2.1.0+cu121)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1)\n",
            "  Downloading spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2023.12.25)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.4.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (1.3.0)\n",
            "Installing collected packages: tokenizers, spacy-alignments, transformers, spacy-transformers, en-core-web-trf\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.1\n",
            "    Uninstalling transformers-4.38.1:\n",
            "      Successfully uninstalled transformers-4.38.1\n",
            "Successfully installed en-core-web-trf-3.6.1 spacy-alignments-0.9.1 spacy-transformers-1.2.5 tokenizers-0.13.3 transformers-4.30.2\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fUhgep3hK-XZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.pipeline import Sentencizer\n",
        "from nervaluate import Evaluator\n",
        "import bs4\n",
        "from print_dict import pd as pdic\n",
        "import requests\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Span\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "from textwrap import wrap\n",
        "import os.path\n",
        "import functools\n",
        "import operator\n",
        "from collections import deque\n",
        "from operator import itemgetter\n",
        "from spacy.symbols import nsubj, VERB, AUX\n",
        "\n",
        "# spaczz lirrary for Similarity Matching\n",
        "from spaczz.matcher import SimilarityMatcher\n",
        "\n",
        "# RDFLib libraries\n",
        "from rdflib import Graph\n",
        "import pprint\n",
        "from rdflib import RDFS\n",
        "from rdflib import URIRef\n",
        "from rdflib.namespace import RDF\n",
        "\n",
        "# Colab specific libraries\n",
        "if isColab:\n",
        "  from google.colab import files\n",
        "\n",
        "# General Libraries\n",
        "import pandas as pd\n",
        "import srsly\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Spacy Related Imports\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding, compile_infix_regex, get_words_and_spaces\n",
        "from spacy.tokens import Span, DocBin, Doc\n",
        "from spacy.vocab import Vocab\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.training import Example\n",
        "# import spacy_sentence_bert\n",
        "\n",
        "# Ignore OOV warnings\n",
        "warnings.filterwarnings(\"ignore\", message=r\"\\[W008\\]\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading Disease A-Z Datasets"
      ],
      "metadata": {
        "id": "dNbIXSCaplIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some of the Directories... Please also check the Main function at the bottom\n",
        "COLAB_DIR = \"\"\n",
        "if isColab:\n",
        "  COLAB_DIR = \"/content/\"\n",
        "\n",
        "# SCHEMA_FILE = COLAB_DIR+\"data/Schema/Disease_KG_Extended.ttl\"\n",
        "STRUCTURED_DATA_DIR = COLAB_DIR+\"data/Structured_Data\"\n",
        "MATCHER_DIR = COLAB_DIR+\"data/Matcher\"\n",
        "CONFIG_DIR = COLAB_DIR+'data/Config'"
      ],
      "metadata": {
        "id": "dP5QwTYkH7J2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/data\"\n",
        "if not os.path.exists(data_path):\n",
        "  '''Downloading the Dataset into CoLab temporary directory'''\n",
        "  !mkdir /content/data\n",
        "\n",
        "  !wget -P /content/data/ 'https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/Résumé/Annotated_Text.zip'\n",
        "  !wget -P /content/data/ 'https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/Résumé/Structured_Data.zip'\n",
        "  !wget -P /content/data/ 'https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/Résumé/Config.zip'\n",
        "\n",
        "  # unzipping the Dataset contents\n",
        "  !unzip \"/content/data/Annotated_Text.zip\" -d \"/content/data/Annotated_Text/\" && rm \"/content/data/Annotated_Text.zip\"\n",
        "  !unzip \"/content/data/Structured_Data.zip\" -d \"/content/data/Structured_Data/\" && rm \"/content/data/Structured_Data.zip\"\n",
        "  !unzip \"/content/data/Config.zip\" -d \"/content/data/Config\" && rm \"/content/data/Config.zip\""
      ],
      "metadata": {
        "id": "x2Ifhur9YGdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0101fb21-dc24-4238-ef67-0496f1bc1397"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-05 20:08:59--  https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/R%C3%A9sum%C3%A9/Annotated_Text.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247831 (242K) [application/zip]\n",
            "Saving to: ‘/content/data/Annotated_Text.zip’\n",
            "\n",
            "\rAnnotated_Text.zip    0%[                    ]       0  --.-KB/s               \rAnnotated_Text.zip  100%[===================>] 242.02K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-03-05 20:08:59 (5.65 MB/s) - ‘/content/data/Annotated_Text.zip’ saved [247831/247831]\n",
            "\n",
            "--2024-03-05 20:08:59--  https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/R%C3%A9sum%C3%A9/Structured_Data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25822 (25K) [application/zip]\n",
            "Saving to: ‘/content/data/Structured_Data.zip’\n",
            "\n",
            "Structured_Data.zip 100%[===================>]  25.22K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-03-05 20:09:00 (9.34 MB/s) - ‘/content/data/Structured_Data.zip’ saved [25822/25822]\n",
            "\n",
            "--2024-03-05 20:09:00--  https://raw.githubusercontent.com/dtim-upc/THOR/main/Dataset/R%C3%A9sum%C3%A9/Config.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 472 [application/zip]\n",
            "Saving to: ‘/content/data/Config.zip’\n",
            "\n",
            "Config.zip          100%[===================>]     472  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-05 20:09:00 (28.3 MB/s) - ‘/content/data/Config.zip’ saved [472/472]\n",
            "\n",
            "Archive:  /content/data/Annotated_Text.zip\n",
            "  inflating: /content/data/Annotated_Text/test/test_20.jsonl  \n",
            "  inflating: /content/data/Annotated_Text/train/train.jsonl  \n",
            "  inflating: /content/data/Annotated_Text/val/valid.jsonl  \n",
            "Archive:  /content/data/Structured_Data.zip\n",
            "  inflating: /content/data/Structured_Data/awards.csv  \n",
            "  inflating: /content/data/Structured_Data/certification.csv  \n",
            "  inflating: /content/data/Structured_Data/college_name.csv  \n",
            "  inflating: /content/data/Structured_Data/companies_worked_at.csv  \n",
            "  inflating: /content/data/Structured_Data/degrees.csv  \n",
            "  inflating: /content/data/Structured_Data/language.csv  \n",
            "  inflating: /content/data/Structured_Data/location.csv  \n",
            "  inflating: /content/data/Structured_Data/name.csv  \n",
            "  inflating: /content/data/Structured_Data/skills.csv  \n",
            "  inflating: /content/data/Structured_Data/university.csv  \n",
            "  inflating: /content/data/Structured_Data/worked_as.csv  \n",
            "  inflating: /content/data/Structured_Data/years_of_experience.csv  \n",
            "Archive:  /content/data/Config.zip\n",
            "  inflating: /content/data/Config/label_config_Entity.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_out_dir(OUTPUT_DIR):\n",
        "  '''This function will be used to remove the outputs in different run'''\n",
        "  if os.path.exists(OUTPUT_DIR):\n",
        "    shutil.rmtree(OUTPUT_DIR)\n",
        "\n",
        "  os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "RhHDedZfceMV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Conversion Part"
      ],
      "metadata": {
        "id": "InQ4__x1UkH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_entity_spans(text, spans):\n",
        "  '''Data Cleaning: Removes leading and trailing white spaces from entity spans.'''\n",
        "  invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "  valid_spans = []\n",
        "  for start, end, label in spans:\n",
        "    valid_start = start\n",
        "    valid_end = end\n",
        "    while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
        "      valid_start += 1\n",
        "    while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
        "      valid_end -= 1\n",
        "\n",
        "    if valid_start < valid_end:\n",
        "      valid_spans.append((valid_start, valid_end, label))\n",
        "\n",
        "  return valid_spans"
      ],
      "metadata": {
        "id": "BCAkc7AYUrnM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_entities(entities):\n",
        "  ''''This function will remove overlapping spans'''\n",
        "  entities_copy = entities.copy()\n",
        "\n",
        "  # append entity only if it is longer than its overlapping entity\n",
        "  i = 0\n",
        "  for entity in entities_copy:\n",
        "      j = 0\n",
        "      for overlapping_entity in entities_copy:\n",
        "          # Skip self\n",
        "          if i != j:\n",
        "              e_start, e_end, oe_start, oe_end = entity[0], entity[1], overlapping_entity[0], overlapping_entity[1]\n",
        "              # Delete any entity that overlaps, keep if longer\n",
        "              if ((e_start >= oe_start and e_start <= oe_end) \\\n",
        "              or (e_end <= oe_end and e_end >= oe_start)) \\\n",
        "              and ((e_end - e_start) <= (oe_end - oe_start)):\n",
        "                  entities.remove(entity)\n",
        "          j += 1\n",
        "      i += 1\n",
        "\n",
        "  return entities"
      ],
      "metadata": {
        "id": "wo3W85mMuQxe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def docanno_to_spacy_ner_db(DATA_DIR):\n",
        "  '''\n",
        "  This function takes a directory of docanno annotated datasets for NER/RE\n",
        "  and converts them into SpaCy DocBin Object which is Trainable via commandline\n",
        "  '''\n",
        "  # Creates a blank Tokenizer with just the English vocab\n",
        "  nlp = spacy.blank(\"en\")\n",
        "\n",
        "  Doc.set_extension(\"rel\", default={},force=True)\n",
        "  vocab = Vocab()\n",
        "\n",
        "  word_count = 0\n",
        "  no_files = 0\n",
        "  no_doc = 0\n",
        "  no_entities = 0\n",
        "  error_cnt = 0\n",
        "\n",
        "  # the DocBin will store the example documents\n",
        "  db = DocBin()\n",
        "\n",
        "  for dirname, _, filenames in os.walk(DATA_DIR):\n",
        "    for filename in filenames:\n",
        "      file_path = os.path.join(dirname, filename)\n",
        "\n",
        "      try:\n",
        "        \"\"\" Iterate through the Jsonl file to create serialize Docbin object / .spacy IOB File \"\"\"\n",
        "        for json_line in srsly.read_jsonl(file_path):\n",
        "\n",
        "          # parsing the docanno JSON data (per-line)\n",
        "          text = json_line[\"text\"]\n",
        "          entities = json_line[\"label\"]\n",
        "          # id = json_line[\"id\"]\n",
        "\n",
        "          entities = clean_entities(entities)\n",
        "\n",
        "          new_spans = []\n",
        "          for span in entities:\n",
        "            new_spans.append((span[0], span[1], span[2]))\n",
        "\n",
        "          # cleaning and validating the leading and trailing spaces from the annotated entities\n",
        "          valid_spans = trim_entity_spans(text, new_spans)\n",
        "\n",
        "          \"\"\" Parsing tokens from Text \"\"\"\n",
        "          tokens = nlp(text)\n",
        "\n",
        "          entities = []\n",
        "\n",
        "          spaces = [True if tok.whitespace_ else False for tok in tokens]\n",
        "          words = [t.text for t in tokens]\n",
        "          doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "          for start, end, label in valid_spans:\n",
        "            \"\"\" The modes should be: strict, contract, and expand \"\"\"\n",
        "            # print(eg['text'][int(span[\"start_offset\"]):int(span[\"end_offset\"])])\n",
        "            entity = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
        "\n",
        "            # Not considering the spans which are Erroneous\n",
        "            if entity is None:\n",
        "              error_cnt += 1\n",
        "              # print(tokens)\n",
        "              # file_name = ttext.split('\\n')[0]\n",
        "              # print(f'Error Found in ID: {id} -- Span: {(start, end, label)} -- Part: {tokens[start: end]}')\n",
        "\n",
        "            else:\n",
        "              no_entities += 1\n",
        "              entities.append(entity)\n",
        "\n",
        "          # print(entities)\n",
        "          try:\n",
        "            doc.ents = entities\n",
        "            word_count += len(words)-int(7)\n",
        "          except:\n",
        "            print()\n",
        "            # print(entities)\n",
        "            print(f\"Cannot Assign Entities\")\n",
        "            # print(f\"Cannot Assign Entities for ID: {id}\")\n",
        "            continue\n",
        "\n",
        "          db.add(doc)\n",
        "          no_doc += 1\n",
        "\n",
        "      except:\n",
        "        print('Error While Loading JSON Data From Input Directory. Please check if you have other file type...')\n",
        "\n",
        "      no_files +=1\n",
        "  print(f\"- Files: {no_files} \\n- Processed Documents: {no_doc} \\n- Total Entities: {no_entities} \\n- Erroneous Entities (Ignored): {error_cnt} \\n- Total Words: {word_count}\")\n",
        "\n",
        "  return doc, db"
      ],
      "metadata": {
        "id": "s18Rxcy3UuSR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_from_docbin(file_path: str, nlp):\n",
        "  '''This function loads data from SpaCy docbin formatted files into spacy compitable JSON format'''\n",
        "  doc_bin = DocBin().from_disk(file_path)\n",
        "  samples, entities_count = [], 0\n",
        "  for doc in doc_bin.get_docs(nlp.vocab):\n",
        "    sample = {\n",
        "      \"text\": doc.text,\n",
        "      \"entities\": []\n",
        "    }\n",
        "    if len(doc.ents) > 0:\n",
        "      # print(doc.ents)\n",
        "      entities = [(e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
        "      sample[\"entities\"] = entities\n",
        "      entities_count += len(entities)\n",
        "    else:\n",
        "      warnings.warn(\"Sample without entities!\")\n",
        "    samples.append(sample)\n",
        "  return samples, entities_count"
      ],
      "metadata": {
        "id": "5ygazl30VHTT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading Data"
      ],
      "metadata": {
        "id": "EpRbrkfFjU8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_trf\n",
        "# TODO - we can aditionally remove the unnecessary pipeline from it -- **speed improvement**\n",
        "nlp_trf = en_core_web_trf.load()"
      ],
      "metadata": {
        "id": "SHey33Q1Nnna"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Using custom Word Vector Models\n",
        "# if isColab:\n",
        "#   nlp = spacy.load(\"/content/wiki-news-300d-1M-subword\")\n",
        "# else:\n",
        "#   nlp = spacy.load(\"Embeddings/glove-840B-300d\")"
      ],
      "metadata": {
        "id": "k_ZkNY2vNjur"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '''Import the appropriate model and adding the extra pipeline needed'''\n",
        "# # import en_core_web_md\n",
        "import en_core_web_lg\n",
        "\n",
        "nlp = en_core_web_lg.load()\n",
        "\n",
        "# # sbd = nlp.create_pipe('sentencizer')\n",
        "nlp.add_pipe('sentencizer')"
      ],
      "metadata": {
        "id": "qjgX9rbuBqQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd108c7b-4b92-4d1f-a81a-23c1aa4d26c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.pipeline.sentencizer.Sentencizer at 0x7f5af6269440>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This is just to SHOW the Named Entities - No Real Purpose \"\"\"\n",
        "ENTITY_LABELS = []\n",
        "for lbl in srsly.read_json(CONFIG_DIR + '/label_config_Entity.json'):\n",
        "  ENTITY_LABELS.append(lbl['text'])\n",
        "print(ENTITY_LABELS)"
      ],
      "metadata": {
        "id": "1EeFKA0WNrik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5893c4b6-f171-4165-bf76-9d0ee1d45656"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NAME', 'LOCATION', 'COLLEGE NAME', 'SKILLS', 'LANGUAGE', 'WORKED AS', 'YEARS OF EXPERIENCE', 'DEGREE', 'CERTIFICATION', 'UNIVERSITY', 'COMPANIES WORKED AT', 'AWARDS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def input_file(file_name=\"\"):\n",
        "  '''This function uploads a file and retunrs it's filename'''\n",
        "\n",
        "  # if file_name is empty, that means we are using CoLAB Google Hosted Runtime\n",
        "  if not file_name:\n",
        "    input_file = files.upload()\n",
        "    file_name = list(input_file.keys())[0]\n",
        "\n",
        "  # removing the new file if already exits\n",
        "  path = os.path.abspath(file_name.split('.')[0]+' (1).'+file_name.split('.')[1])\n",
        "  if os.path.exists(path) == True:\n",
        "    os.remove(path)\n",
        "\n",
        "  return file_name"
      ],
      "metadata": {
        "id": "0AP7eo4e1JiQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_csv(file_name=\"\"):\n",
        "  '''\n",
        "  This function will get a CSV from the user having TWO COL i.e., Disease_Name, Affected_Anatomy and\n",
        "  returns a Dictionary having a structure: {\"Tuberculosis\": ['lungs', 'brain', 'kidneys', 'spine'], ...}\n",
        "  '''\n",
        "  # if file_name is empty, that means we are using CoLAB Google Hosted Runtime\n",
        "  if not file_name:\n",
        "    file_name = input_file()\n",
        "\n",
        "  structured_data = {}\n",
        "\n",
        "  with open(file_name, 'r', encoding='ISO-8859-1') as file:\n",
        "      csvreader = csv.reader(file)\n",
        "      # we can ignore the header\n",
        "      header = next(csvreader)\n",
        "      # print('Reading New Structured Data Source: {}'.format(file_name.split('\\\\')[1]))\n",
        "      # print('Data Headers: ', header)\n",
        "\n",
        "      for row in csvreader:\n",
        "        # # print(row)\n",
        "        # splits the comma separated values from the 1st column\n",
        "        first_cols = row[0].split(',')\n",
        "\n",
        "        # each of the instances of the first col (subject) will have the same value domain\n",
        "        for instance in first_cols:\n",
        "          instance = instance.strip()\n",
        "\n",
        "          # if we do not have any second column (non-relational data)\n",
        "          try:\n",
        "            # splits the comma separated values from the 2nd column\n",
        "            value_domain = row[1].split(',')\n",
        "            # removes the leading and trailing spaces\n",
        "            value_domain = [x.strip() for x in value_domain]\n",
        "            # making instance/value-domain dictionary\n",
        "            structured_data[instance] = value_domain\n",
        "          except:\n",
        "            structured_data[instance] = []\n",
        "\n",
        "  # print('Instances/Values:')\n",
        "  # print(structured_data)\n",
        "  # print()\n",
        "\n",
        "  return header, structured_data"
      ],
      "metadata": {
        "id": "SQeMzNp80ssQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_structured_data(STRUCTURED_DATA_DIR=STRUCTURED_DATA_DIR):\n",
        "  '''This will read the Structured Data From the CSV Files of a Directory and accumulate them'''\n",
        "  # DATA: gets the structured data from CSV files\n",
        "  accu_data = {}\n",
        "\n",
        "  for dirname, _, filenames in os.walk(STRUCTURED_DATA_DIR):\n",
        "    for filename in filenames:\n",
        "      file_path = os.path.join(dirname, filename)\n",
        "      # # print(file_path)\n",
        "      sd_header, sd_data = get_data_from_csv(file_path)\n",
        "      accu_data = accumulate_unique_data(prev_data=accu_data, keys_list=sd_header, data_dic=sd_data)\n",
        "\n",
        "  # print('\\nAfter Structured Data Accumulation:')\n",
        "  # print(accu_data)\n",
        "\n",
        "  return accu_data"
      ],
      "metadata": {
        "id": "zLTZuSuozoHw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(file_name=\"\"):\n",
        "  '''Getting the TEXT file from the user - and renaming it'''\n",
        "\n",
        "  # if file_name is empty, that means we are using CoLAB Google Hosted Runtime\n",
        "  input_txt = \"\"\n",
        "  if not file_name:\n",
        "    text_file = files.upload()\n",
        "    file_name = list(text_file.keys())[0]\n",
        "    input_txt = text_file[file_name].decode(\"utf-8\").strip()\n",
        "\n",
        "    given_text_file_name = str(file_name)\n",
        "\n",
        "    old_name = COLAB_DIR + given_text_file_name\n",
        "    new_name = COLAB_DIR + 'inference_text.txt'\n",
        "\n",
        "    # removing the new file if already exits\n",
        "    if os.path.exists(new_name) == True:\n",
        "      os.remove(new_name)\n",
        "\n",
        "    os.rename(old_name, new_name)\n",
        "    file_name = new_name\n",
        "\n",
        "  # if the file is called from local runtime with filename specified\n",
        "\n",
        "  with open(file_name, 'r', encoding='utf-8') as text_file:\n",
        "    input_txt = text_file.read().strip()\n",
        "\n",
        "  # print(\"Contents of the Input Text File:\")\n",
        "  # print(input_txt)\n",
        "\n",
        "  return input_txt"
      ],
      "metadata": {
        "id": "jNhRCQEkMFl3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_segmentation(input_txt, print_indx = False):\n",
        "    '''This Function Segments the Text into Sentences and Tokens'''\n",
        "\n",
        "    doc = nlp(input_txt)\n",
        "\n",
        "    sentences = []\n",
        "    # sentences_tokens = []\n",
        "    for sent in doc.sents:\n",
        "        # print(sent)\n",
        "        # here sentences are strings\n",
        "        sentences.append(sent.text.strip())\n",
        "        # here a  particular sentence is a list of strings containing the words/tokens\n",
        "        # sentences_tokens.append([token.text for token in sent])\n",
        "\n",
        "    # print('\\nTotal Sentence Found = ', len(sentences))\n",
        "\n",
        "    if print_indx is not False:\n",
        "        print('\\nExample Sentence and Tokens:')\n",
        "        print(sentences[print_indx])\n",
        "        # print(sentences_tokens[print_indx])\n",
        "\n",
        "    return doc, sentences"
      ],
      "metadata": {
        "id": "i-zCn0yMRKqe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptualization"
      ],
      "metadata": {
        "id": "ktC0mpAf6rw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_unique_data(prev_data, keys_list, data_dic):\n",
        "  ''' This function creates a set (unique lists) of data for each of the Concepts (e.g., Disease, Sysmtoms, Anatomy etc.)\n",
        "      Parameters:\n",
        "        pre_data  -> already created dictionary with other data sources previously\n",
        "        keys_list -> list of keys (column names) for the new data source\n",
        "        data_dic  -> a dictionary holding the source\n",
        "\n",
        "      Returns:  a dictionary having the keys_list as key and a list of unique (set) values from the data source\n",
        "  '''\n",
        "  new_data = {}\n",
        "  # if we have data already residing in the prev_dic\n",
        "  if prev_data:\n",
        "    new_data = prev_data\n",
        "\n",
        "  # get a list from the keys of the data_dic - for example list of disease names\n",
        "  keys_data_list = list(data_dic.keys())\n",
        "\n",
        "  values_data_list = list(data_dic.values())\n",
        "  # flattening the list of lists and taking only a the unique values\n",
        "  values_data_list = list(set(functools.reduce(operator.iconcat, values_data_list)))\n",
        "\n",
        "  # merging the above tow in a single list of lists\n",
        "  data_lists = [keys_data_list, values_data_list]\n",
        "\n",
        "  # Accumulates the new data into Key and Value while merging prev values if the key is already present\n",
        "  for key, val in zip(keys_list, data_lists):\n",
        "    if key in new_data:\n",
        "      pre_val = new_data[key]\n",
        "      # # print(pre_val)\n",
        "      val = list(set(functools.reduce(operator.iconcat, [val + pre_val])))\n",
        "      # # print(val)\n",
        "\n",
        "    # removing empty string from the instances\n",
        "    if '' in val:\n",
        "      val.remove('')\n",
        "\n",
        "    new_data[key] = val\n",
        "\n",
        "  # # print(new_data,'\\n')\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "QcfJcDbZhNoN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initiate_matcher(patterns, threshold=80):\n",
        "  '''Initializing and Fine-tuning the matcher'''\n",
        "  # For more information of how it works please see: https://github.com/gandersen101/spaczz\n",
        "  # changing min_r2 from default of 75 to produce matches in this example -- using custom vocab doesn't work for now\n",
        "  matcher = SimilarityMatcher(vocab=nlp.vocab, min_r2=threshold)\n",
        "\n",
        "  # UNCOMMENT - this if you set the threshold level to below 50\n",
        "  # matcher = SimilarityMatcher(vocab=nlp.vocab, min_r2=threshold, min_r1=1)\n",
        "\n",
        "  for key_concept in patterns:\n",
        "    # print(key_concept)\n",
        "\n",
        "    pattern = [nlp(instance) for instance in patterns[key_concept]]\n",
        "\n",
        "    # we can add as many patterns as we want with specified names of Keys/Concepts\n",
        "    matcher.add(key_concept, pattern)\n",
        "\n",
        "  return matcher"
      ],
      "metadata": {
        "id": "QsGH_roxCIwF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_noun_chunks(np_chunks, sent):\n",
        "  ''' Removes leading and trailing stop words from noun chunks '''\n",
        "  cleaned_chunks = []\n",
        "  for chunk in np_chunks:\n",
        "    start = chunk.start\n",
        "    end = chunk.end\n",
        "\n",
        "    # Move start and end while they point to a stop word\n",
        "    while start < end and sent[start].is_stop:\n",
        "      start += 1\n",
        "    while start < end and sent[end - 1].is_stop:\n",
        "      end -= 1\n",
        "\n",
        "    # If there's any non-stop word left, create a new span with the new start and end\n",
        "    if start < end:\n",
        "      cleaned_chunks.append(Span(sent, start, end, label=chunk.label))\n",
        "\n",
        "  return cleaned_chunks"
      ],
      "metadata": {
        "id": "khrIU0xS7oky"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_noun_chunks(SENTENCES):\n",
        "  ''' This function iterates over the sentences and splits them into NP (Noun Chunks)\n",
        "  '''\n",
        "  # doc = nlp(\"Typical symptoms of active Tuberculosis are chronic cough with blood-containing mucus, fever, night sweats, and weight loss.\")\n",
        "  # NP per sentence {indx:(sen_indx, [chunk1, chunk1,...])}\n",
        "  np_sentences = {}\n",
        "\n",
        "  for indx, sentence in enumerate(SENTENCES):\n",
        "    # print(\"\\nSENTENCE-{}: {}\\n\".format(indx, sentence))\n",
        "    # This is using Transformer Based Model to generate better Dependency Parsing.\n",
        "    sent = nlp_trf(sentence)\n",
        "\n",
        "    # Extracts noun chunks possibly with leading and trailing stop words from the sentence\n",
        "    np_chunks = [chunk for chunk in sent.noun_chunks]\n",
        "\n",
        "    # We are cleaning the chunks from leading/trailing/fully-containing stop words\n",
        "    # clean_np_chunks = strip_noun_chunks(np_chunks, sent)\n",
        "\n",
        "    np_sentences[indx] = (sent, np_chunks)\n",
        "\n",
        "  return np_sentences"
      ],
      "metadata": {
        "id": "I3-e6OBFvPah"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "import lemminflect\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def sequence_similarity(phrase, pattern):\n",
        "  return SequenceMatcher(None, phrase, pattern).ratio()\n",
        "\n",
        "def word_overlap_lemma(phrase, pattern):\n",
        "  '''This function computes the no. of overlaps between the lemmatized tokens of two nlp objects '''\n",
        "  phrase_words_set = {token._.lemma().lower() for token in phrase if not token.is_punct}\n",
        "  pattern_words_set = {token._.lemma().lower() for token in pattern if not token.is_punct}\n",
        "\n",
        "  # score = Intersection / Union (similar to IoU score)\n",
        "  # The & operator gives the intersection of two sets, while | gives set union\n",
        "  word_overlap_score = (len(phrase_words_set & pattern_words_set)/len(phrase_words_set | pattern_words_set))\n",
        "\n",
        "  return word_overlap_score\n",
        "\n",
        "def word_overlap_stem(phrase, pattern):\n",
        "  '''This function computes the no. of overlaps between the lemmatized tokens of two nlp objects '''\n",
        "  ps = PorterStemmer()\n",
        "\n",
        "  phrase_words_set = {ps.stem(token.text.lower()) for token in phrase if not token.is_punct}\n",
        "  pattern_words_set = {ps.stem(token.text.lower()) for token in pattern if not token.is_punct}\n",
        "\n",
        "  # score = Intersection / Union (similar to IoU score)\n",
        "  # The & operator gives the intersection of two sets, while | gives set union\n",
        "  word_overlap_score = (len(phrase_words_set & pattern_words_set)/len(phrase_words_set | pattern_words_set))\n",
        "\n",
        "  return word_overlap_score"
      ],
      "metadata": {
        "id": "h9IjtPnMmhaU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_dup_match(matches):\n",
        "  '''This function will remove the matched instances where they have same span and entity type but different confidence score\n",
        "    Example:\n",
        "      (abdomen, 35, 36, Anatomy, 1.0)\n",
        "      (abdomen, 35, 36, Anatomy, 0.5)\n",
        "      (abdomen, 35, 36, Anatomy, 0.3)\n",
        "  '''\n",
        "  filtered_dict = {}\n",
        "\n",
        "  for match_tup in matches:\n",
        "    # We'll create a dictionary with keys as a combination of (match_id, start, end) and values as the original tuple.\n",
        "    match_id, start, end, confidence_score, pattern = match_tup\n",
        "    key = (match_id, start, end)\n",
        "\n",
        "    # If we encounter a tuple with the same (match_id, start, end) but a higher confidence_score, we'll update the dictionary value.\n",
        "    if key not in filtered_dict or confidence_score > filtered_dict[key][3]:\n",
        "      filtered_dict[key] = match_tup\n",
        "\n",
        "  return list(filtered_dict.values())"
      ],
      "metadata": {
        "id": "dL5VR9reILbu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matching_and_ranking(np_sentences, matcher, top_k=1):\n",
        "  ''' Need More Details: This is a Core function that uses Word Vectors internally to match similar concepts...!! '''\n",
        "\n",
        "  # matches per sentence {i:[(np_chunk, matched_sub_chunk, pattern_concept, confidence), ...]}\n",
        "  matched_sentences = {}\n",
        "\n",
        "  for i in np_sentences:\n",
        "\n",
        "    sent, clean_np_chunks = np_sentences[i]\n",
        "    matches_sent = []\n",
        "    # print(f\"\\nSENTENCE-{i+1}: {sent}\")\n",
        "\n",
        "    for chunk in clean_np_chunks:\n",
        "      phrase = nlp(chunk.text)\n",
        "\n",
        "      # TODO: we could apply the Matcher for the whole DOCUMENT - to check **speed improvement**\n",
        "      matches_orig = matcher(phrase)\n",
        "\n",
        "      matches = []\n",
        "\n",
        "      # We go for further ranking (+sorting) using two more matching techniques\n",
        "      for match_id, start, end, ratio, pattern in matches_orig:\n",
        "        # Exception: This is a special case of possessive noun which we need to ignore from the matching results\n",
        "        # if(phrase.text ==\"'s\"):\n",
        "        #   continue\n",
        "\n",
        "        # Word overlaps - providing nlp objects for better tokenization\n",
        "        # word_matched = word_overlap_stem(phrase[start:end], nlp(pattern))\n",
        "\n",
        "        # Gestalt pattern matching - difflib\n",
        "        seq_sim = sequence_similarity(chunk.text, pattern)\n",
        "\n",
        "        # Mean Similarity Score from the 2 matchers - 'ratio' is out of 100 so we normalized it\n",
        "        # confidence_score = (ratio/100  + word_matched + seq_sim)/3\n",
        "        confidence_score = round(ratio/100, 2)\n",
        "\n",
        "        matches.append((match_id, start, end, confidence_score, pattern))\n",
        "\n",
        "      # Removing duplicate matches having same span as well as same entity type (matching id)\n",
        "      matches = remove_dup_match(matches)\n",
        "\n",
        "      # Sorting the list of tuples by 'confidence score'\n",
        "      matches.sort(key=itemgetter(3), reverse=True)\n",
        "\n",
        "      # TODO: For now we are only taking the TOP-3 matching results.\n",
        "      for match_id, match_start, match_end, conf, pattern in matches[0:top_k]:\n",
        "        # This is to track down the matches within the whole Sentence\n",
        "        sent_start = chunk.start + match_start\n",
        "        sent_end = chunk.start + match_end\n",
        "\n",
        "        try:\n",
        "          matched_sub_chunk = Span(sent, sent_start, sent_end, label=chunk.label)\n",
        "        except:\n",
        "          print(f'Error Creating Span in Sentence {i}\\n{sent}')\n",
        "          continue\n",
        "\n",
        "        # TODO: If we need we can also include the NP (chunk) here for future use\n",
        "        matches_sent.append((chunk, matched_sub_chunk, match_id, conf))\n",
        "\n",
        "        # print(f'Chunk = {chunk}, Match Start = {match_start}, Match End = {match_end}, Matched Sub-Chunk = {matched_sub_chunk}, Concept = {match_id}, Confidence = {conf}, Pattern = {pattern}')\n",
        "\n",
        "    # Adding the (sent, matches) to the dictionary with Sentence Index as Key for return purpose\n",
        "    matched_sentences[i] = (sent, matches_sent)\n",
        "\n",
        "  return matched_sentences"
      ],
      "metadata": {
        "id": "eu2ENipU-OVy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_match(matched_sentences):\n",
        "  # print('Final List of Concepts in this Sentence:\\n')\n",
        "\n",
        "  count = 0\n",
        "  for S_i in matched_sentences:\n",
        "    sent, matches = matched_sentences[S_i]\n",
        "\n",
        "    print(\"Sentence-{}: '{}'\".format(S_i, sent))\n",
        "    # print(matches)\n",
        "\n",
        "    for np_chunk, matched_sub_chunk, match_id, conf in matches:\n",
        "      print(f'\\t\\tNP Chunk = {np_chunk}, Matched Sub-Chunk = {matched_sub_chunk}, Match Start = {matched_sub_chunk.start}, Match End = {matched_sub_chunk.end}, Concept = {match_id}, Confidence = {conf}')\n",
        "      count += 1\n",
        "    print()\n",
        "\n",
        "  print(f'\\nTotal Entities Recognized in Sentences = {count}')"
      ],
      "metadata": {
        "id": "pxWD_Ejyux0Q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conceptualized_preprocessed_doc(prep_doc, matched_sentences):\n",
        "  ''' This function will map the sentence based matching results into the coreferred documents.\n",
        "      The chunks produces here uses token based span according to the pre-processed document as a whole.\n",
        "\n",
        "      Returns a list of 3 tuples (np_chunk_doc, matched_sub_chunk_doc, match_id, conf)\n",
        "      --> [(Abdominal aortic aneurysm, Abdominal aortic aneurysm, 'Disease', 1.0), (Abdominal aortic aneurysm, Abdominal, 'Anatomy', 0.62), ...]\n",
        "  '''\n",
        "\n",
        "  entities_pre_doc = []\n",
        "  for i, sent in enumerate(prep_doc.sents):\n",
        "    sent_from_matcher, matched_results = matched_sentences[i]\n",
        "\n",
        "    for np_chunk, matched_sub_chunk, match_id, conf in matched_results:\n",
        "      # Adjust the start and end indices of NP Chunk according to the doc\n",
        "      np_chunk_doc_start = sent.start + np_chunk.start\n",
        "      np_chunk_doc_end = sent.start + np_chunk.end\n",
        "\n",
        "      # Adjust the start and end indices of matched sub-chunk according to the doc\n",
        "      sub_chunk_doc_start = sent.start + matched_sub_chunk.start\n",
        "      sub_chunk_doc_end = sent.start + matched_sub_chunk.end\n",
        "\n",
        "      # Making token based span according to the doc\n",
        "      np_chunk_doc = Span(prep_doc, np_chunk_doc_start, np_chunk_doc_end, label=np_chunk.label)\n",
        "      matched_sub_chunk_doc = Span(prep_doc, sub_chunk_doc_start, sub_chunk_doc_end, label=matched_sub_chunk.label)\n",
        "      entities_pre_doc.append((np_chunk_doc, matched_sub_chunk_doc, match_id, conf))\n",
        "\n",
        "      # print(matched_sub_chunk, matched_sub_chunk_doc)\n",
        "\n",
        "  # print(f'\\nTotal Entities Mapped in Document = {len(entities_pre_doc)}')\n",
        "  return entities_pre_doc"
      ],
      "metadata": {
        "id": "3HNW3GBqJZjk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_char_span(orig_doc, input_txt, entities_pre_doc, tag_np_chunk_only = False):\n",
        "  '''This function conversts token span into character span for the evaluation script to work.\n",
        "     You can choose either to tag the NP Chunk or to be more granular by tagging with Sub-Chunk level matching (default).\n",
        "     The original document object is also populated with the token span so that we can visualize the results of our prediction\n",
        "  '''\n",
        "\n",
        "  entities_span = []\n",
        "  doc_span = []\n",
        "\n",
        "  if tag_np_chunk_only:\n",
        "    for np_chunk_doc, matched_sub_chunk_doc, match_id, conf in entities_pre_doc:\n",
        "      # This will be used for SemEval Evaluation\n",
        "      entities_span.append((np_chunk_doc.start_char, np_chunk_doc.end_char, match_id))\n",
        "      # This will be used to visualize the Span\n",
        "      doc_span.append(Span(orig_doc, np_chunk_doc.start, np_chunk_doc.end, match_id))\n",
        "  else:\n",
        "    for np_chunk_doc, matched_sub_chunk_doc, match_id, conf in entities_pre_doc:\n",
        "      # This will be used for SemEval Evaluation\n",
        "      entities_span.append((matched_sub_chunk_doc.start_char, matched_sub_chunk_doc.end_char, match_id))\n",
        "      # This will be used to visualize the Span\n",
        "      doc_span.append(Span(orig_doc, matched_sub_chunk_doc.start, matched_sub_chunk_doc.end, match_id))\n",
        "\n",
        "  # A dictionary in the format {'text': 'Tuberculosis generally damages the lungs', 'entities': [(0, 12, 'Disease_E'), (35, 40, 'Anatomy_E')]}\n",
        "  ner_prediction = {'text': input_txt, 'entities': entities_span}\n",
        "  orig_doc.spans[\"sc\"] = doc_span\n",
        "\n",
        "  return orig_doc, ner_prediction"
      ],
      "metadata": {
        "id": "otK3hsiA74t8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_spacy_ner_doc(ner_pred):\n",
        "  '''\n",
        "  This function takes a list of directory of NER predictions of the form\n",
        "  {'text': '...', 'entities':[(start, end, tag)]} and converts them into SpaCy Doc Object\n",
        "  '''\n",
        "  # Creates a blank Tokenizer with just the English vocab\n",
        "  nlp = spacy.blank(\"en\")\n",
        "\n",
        "  Doc.set_extension(\"rel\", default={},force=True)\n",
        "  vocab = Vocab()\n",
        "\n",
        "  # try:\n",
        "  # parsing the docanno JSON data (per-line)\n",
        "  text = ner_pred[\"text\"]\n",
        "  spans = ner_pred[\"entities\"]\n",
        "\n",
        "  \"\"\" Parsing tokens from Text \"\"\"\n",
        "  tokens = nlp(text)\n",
        "\n",
        "  entities = []\n",
        "\n",
        "  spaces = [True if tok.whitespace_ else False for tok in tokens]\n",
        "  words = [t.text for t in tokens]\n",
        "  doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "  for start, end, label in spans:\n",
        "    \"\"\" The modes should be: strict, contract, and expand \"\"\"\n",
        "      # print(eg['text'][int(span[\"start_offset\"]):int(span[\"end_offset\"])])\n",
        "    entity = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
        "\n",
        "    # Not considering the spans which are Erroneous\n",
        "    if entity is None:\n",
        "      # file_name = text.split('\\n')[0]\n",
        "      # print(f'No Entity Found in File: {file_name};\\n Span = {start}-{end}; Phrase = {doc.text[start:end]}; Label = {label}\\n')\n",
        "      continue\n",
        "    else:\n",
        "      entities.append(entity)\n",
        "\n",
        "  # print(entities[0].label_)\n",
        "  try:\n",
        "    doc.ents = entities\n",
        "  except:\n",
        "    print(\"=>> Error\")\n",
        "    print(text)\n",
        "\n",
        "  # except:\n",
        "  #   print('Error While Loading Predicted List...')\n",
        "\n",
        "  return doc"
      ],
      "metadata": {
        "id": "1rL4VUMArPxu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Assigns different colors to the Entities during visualization.'''\n",
        "\n",
        "color_list = ['yellow', 'white', 'orange', '#008000', '#800000', '#0D9CB4', '#5813C7', '#0D350E', '#1AA436',\n",
        "          '#1AE0F9', '#BADCA1', '#78A2E5', '#D845FB', '#54B69E', '#800080', '#FF00FF', '#000080']\n",
        "\n",
        "colors = dict(zip(ENTITY_LABELS, color_list))\n",
        "options = {\"colors\": colors}"
      ],
      "metadata": {
        "id": "uVqNZk4vJRmg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_pred_span_cat(doc):\n",
        "  '''Desired Format: doc.spans[\"sc\"] = [Span(doc, 3, 6, \"ORG\"), Span(doc, 5, 6, \"GPE\")]\n",
        "  '''\n",
        "  spacy.displacy.render(doc, style=\"span\", options=options, jupyter=True)"
      ],
      "metadata": {
        "id": "5X5IdFY4A6Pi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_groud_truth_entities(doc):\n",
        "  spacy.displacy.render(doc, style=\"ent\", options=options, jupyter=True)"
      ],
      "metadata": {
        "id": "0X3Zmm_SJWKO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(ner_predictions, filename, semeval_format=True):\n",
        "  # Saving the predictions as JSON - each dictionary on a line\n",
        "  semeval_ent = []\n",
        "  with open(OUTPUT_DIR+'/'+filename, 'w', encoding='UTF-8') as json_file:\n",
        "    for pred in ner_predictions:\n",
        "      if semeval_format:\n",
        "        # prodigy format to work with nereval library - for SemEval 2013 - 9.1 task.\n",
        "        tmp_ent = []\n",
        "        for ent in pred['entities']:\n",
        "          # saved in this format: [{\"label\": \"PER\", \"start\": 2, \"end\": 4}, ... ]\n",
        "          tmp_ent.append({\"label\": ent[2], \"start\": ent[0], \"end\": ent[1]})\n",
        "\n",
        "        semeval_ent.append(tmp_ent)\n",
        "\n",
        "      else:\n",
        "        # this is regullar spacy format, can be used for spacy's default evaluation later\n",
        "        json_file.write(json.dumps(pred, ensure_ascii=False))\n",
        "        json_file.write('\\n')\n",
        "\n",
        "    if semeval_format:\n",
        "      # dumping it into a JSON file\n",
        "      json_file.write(json.dumps(semeval_ent, ensure_ascii=False))\n",
        "\n",
        "  return semeval_ent\n",
        "  # # This is single line JSON Dump of the entile list of dictionary - parser cannot parse it directly\n",
        "  # with open(OUTPUT_DIR+'/predition.jsonl', 'w') as fout:\n",
        "  #     json.dump(ner_predictions, fout)"
      ],
      "metadata": {
        "id": "TKvCpbC5ov7I"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_results(results_by_tag):\n",
        "  '''This fucntion is used to process the individual entity based scores together'''\n",
        "  results_by_entity = []\n",
        "  for entity in ENTITY_LABELS:\n",
        "    df = pd.DataFrame(results_by_tag[entity])\n",
        "    df = df.round(decimals = 2)\n",
        "    df.insert(0,'Entity','')\n",
        "    df['Entity'] = entity\n",
        "    results_by_entity.append(df)\n",
        "\n",
        "  return results_by_entity"
      ],
      "metadata": {
        "id": "QPUwmNTfu0Tw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semeval_evaluation(true, pred):\n",
        "    evaluator = Evaluator(true, pred, tags=ENTITY_LABELS)\n",
        "    results, results_by_tag = evaluator.evaluate()\n",
        "\n",
        "    results = pd.DataFrame(results)\n",
        "    results.to_excel(OUTPUT_DIR+'/'+'overall_benchmark.xlsx')\n",
        "\n",
        "    results_by_entity = pd.concat(process_results(results_by_tag))\n",
        "    results_by_entity.to_excel(OUTPUT_DIR+'/'+'entity_benchmark.xlsx')\n",
        "\n",
        "    return results, results_by_entity"
      ],
      "metadata": {
        "id": "6FlqR5Dqt_nP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import glob\n",
        "\n",
        "def save_matcher(matcher_file):\n",
        "  with open(matcher_file, 'wb') as out_dir:\n",
        "      pickle.dump(matcher, out_dir, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_matcher(matcher_dir):\n",
        "  matcher_file = glob.glob(os.path.join(matcher_dir, '*.pkl'))[0]\n",
        "  with open(matcher_file, 'rb') as inp_dir:\n",
        "    matcher = pickle.load(inp_dir)\n",
        "  return matcher"
      ],
      "metadata": {
        "id": "GBMfpPd_rX2z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "_x4dRIoBxInd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "\n",
        "  # Prompting for evaluation set - validation/test\n",
        "  EVAL_SET = ['valid', 'test', 'train', 'inf']\n",
        "  EVAL_SET = EVAL_SET[int(input('Chose Dataset for Evaluation:\\n[0] Validation\\n[1] Test\\n[2] Training\\n[3] Inferencing\\nEnter your choice:'))]\n",
        "\n",
        "  # Input/Output Directories\n",
        "  INPUT_DIR = \"\"\n",
        "  OUTPUT_DIR = \"\"\n",
        "\n",
        "  if EVAL_SET == 'inf':\n",
        "    if isColab:\n",
        "      input_text = get_text()\n",
        "    else:\n",
        "      # uploaded_text = files.upload()\n",
        "      # text_filename = inference_file(uploaded_text)\n",
        "      input_text = get_text(\"data/Annotated_Text/single.txt\")\n",
        "\n",
        "    samples = [{'text':input_text}]\n",
        "    # print(input_text)\n",
        "  else:\n",
        "    # Input/Output Directories\n",
        "    INPUT_DIR = f'{COLAB_DIR}data/Annotated_Text/{EVAL_SET}'\n",
        "    OUTPUT_DIR = f'{COLAB_DIR}output/{EVAL_SET}'\n",
        "    create_out_dir(OUTPUT_DIR)\n",
        "\n",
        "    print('\\nPreparing Evaluation Dataset:')\n",
        "    doc_valid, db_valid = docanno_to_spacy_ner_db(INPUT_DIR)\n",
        "    db_valid.to_disk(OUTPUT_DIR + f\"/disease_A-Z_{EVAL_SET}.spacy\")\n",
        "\n",
        "    # reading the ground truth entities with text file from spacy docbin\n",
        "    samples, ground_entities_count = load_json_from_docbin(OUTPUT_DIR + f\"/disease_A-Z_{EVAL_SET}.spacy\", nlp)\n",
        "\n",
        "  # reading and accumulating (integrated) structured data\n",
        "  accu_data = accumulate_structured_data()\n",
        "  # pdic(accu_data)\n",
        "  # initializing and fine-tuning the matcher\n",
        "\n",
        "  if not os.path.exists(MATCHER_DIR):\n",
        "    create_out_dir(MATCHER_DIR)\n",
        "    print('Fine-tuning the matcher with structured data...')\n",
        "    # Change the threshold here to be more precision/recall oriented\n",
        "    threshold=100\n",
        "    matcher = initiate_matcher(patterns=accu_data, threshold = threshold)\n",
        "\n",
        "    # saving the matcher for future usecases... reducing time\n",
        "    save_matcher(f'{MATCHER_DIR}/matcher_{threshold}.pkl')\n",
        "\n",
        "  else:\n",
        "    print('Loading the matcher from directory...')\n",
        "    matcher = load_matcher(MATCHER_DIR)\n",
        "\n",
        "  ner_predictions = []\n",
        "  all_docs = []\n",
        "  count = 0\n",
        "  total_doc = len(samples)\n",
        "\n",
        "  print('\\nConceptualization Process Started...')\n",
        "  # each sample represents one document..\n",
        "  for ground in samples:\n",
        "    input_txt = ground['text']\n",
        "    # print(f'\\n{input_txt}')\n",
        "\n",
        "    # Segmenting the document into sentences - for Conceptualization we can also process the whole document\n",
        "    doc, sentences = sentence_segmentation(input_txt, print_indx=False)\n",
        "\n",
        "    # Detecting the NP Chunks - also removes leading and trailing stop words in the NP chunks\n",
        "    np_sentences = identify_noun_chunks(sentences)\n",
        "    # print(np_sentences)\n",
        "\n",
        "    # Getting the initial Top-k matrching results\n",
        "    matched_sentences = matching_and_ranking(np_sentences, matcher=matcher, top_k=1)\n",
        "    # print_match(matched_sentences)\n",
        "\n",
        "    # Conceptualizing the Text Document - token based indices\n",
        "    entities_pre_doc = conceptualized_preprocessed_doc(doc, matched_sentences)\n",
        "    # print(f'\\nEntities in the Coreference Resolved Document:\\n{entities_pre_doc}')\n",
        "\n",
        "    # Organizing indexes of those entitties according to the original text, also creating span categories for visualization\n",
        "    doc, ner_pred_tmp = get_char_span(doc, input_txt, entities_pre_doc, tag_np_chunk_only = True)\n",
        "    # print(f'\\nNER Prediction for current document:\\n{ner_pred_tmp}')\n",
        "    ner_predictions.append(ner_pred_tmp)\n",
        "    all_docs.append(doc)\n",
        "\n",
        "    count +=1\n",
        "    # print(f'Document-{count}')\n",
        "    if count % 10 == 0:\n",
        "      print(f'{count}/{total_doc} Document Processed...')\n",
        "\n",
        "  if EVAL_SET == 'inf':\n",
        "    # Visualizing the Predictions for the Given Input Text\n",
        "    print('\\n########### Prediction ###########\\n')\n",
        "    render_pred_span_cat(all_docs[0])\n",
        "\n",
        "  else:\n",
        "    # saving the grond and predictions into a JSONL file for later evaluation.\n",
        "    semeval_ground = save_predictions(samples, filename='ground.jsonl')\n",
        "    semeval_pred = save_predictions(ner_predictions, filename='prediction.jsonl')\n",
        "\n",
        "    # doing the evaluation following SemEval 2013 metrics\n",
        "    results, results_by_entity = semeval_evaluation(true=semeval_ground, pred=semeval_pred)\n",
        "\n",
        "    # Saving this for Future Experiments... Spacy Format\n",
        "    _ = save_predictions(ner_predictions, filename='prediction_THOR_spacy.jsonl', semeval_format=False)\n",
        "\n",
        "    print('\\n########### Overall Results ###########\\n')\n",
        "    print(f\"Precision: {results['partial']['precision']}\\nRecall: {results['partial']['recall']}\\nF1: {results['partial']['f1']}\\n\")"
      ],
      "metadata": {
        "id": "1OzJAh7XDTVA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}